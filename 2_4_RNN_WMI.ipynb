{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Introduction\n",
    "\n",
    "\n",
    "Up until now, the layers we have studed (Dense, Convolution) are primarily used to implement functions\n",
    "that are one to one: \n",
    "- a single input (of fixed length) yields a single output.\n",
    "\n",
    "Recurrent Neural Networks (RNN) deal with sequences: \n",
    "- sequences of inputs and sequence of outputs.\n",
    "\n",
    "Sequences have order: a permutation of the elements of a sequence is a completely distinct sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Order matters !\n",
    "\n",
    "Order matters\n",
    "- set $\\{ \\x_{(1)} \\dots \\x_{(\\tt-1)}\\}$ versus sequence $\\x_{(1)} \\dots \\x_{(\\tt-1)}$\n",
    "    - order doesn't matter for a set\n",
    "    - $\\{ \\x_{(1)} \\dots \\x_{(\\tt-1)}\\} = \\{  \\x_{\\tt-1)} \\ldots \\x_{(1)}\\}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Same prices</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_sequence_1.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<center>Same words</center>\n",
    "$$\n",
    "\\begin{matrix}\n",
    "\\text{Machine} & \\text{Learning} & \\text{is} & \\text{easy} & \\text{not} & \\text{difficult} \\\\\n",
    "\\text{Machine} & \\text{Learning} & \\text{is} & \\text{difficult} & \\text{not} & \\text{easy} \\\\\n",
    "\\end{matrix}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Same pixels</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_sequence_2.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "All pairs of inputs in above examples are\n",
    "- identical as sets\n",
    "- different as sequences\n",
    "\n",
    "**Note**\n",
    "\n",
    "If the paired examples have different labels:\n",
    "- NN will find as subset of features that separate them\n",
    "- the separating feature then becomes an anchor, restricting reordering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Functions on sequence\n",
    "\n",
    "Examples of functions with sequence as inputs but single output (many to one mapping)\n",
    "- predict next value in time series\n",
    "- predict sentiment of text\n",
    "\n",
    "Examples of functions with single input but sequence as output (one to many mapping)\n",
    "- text generation (char-rnn) ?\n",
    "\n",
    "Examples of functions with sequence as inputs and outputs\n",
    "- translating from one language to another\n",
    "- captioning a movie"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Notation alert**\n",
    "\n",
    "Lot's of dimenions !\n",
    "- $\\x^\\ip$ is a sequence with elements $\\x^\\ip_{(1)} \\dots \\x^\\ip_{(\\tt-1)}$\n",
    "- each element $\\x^\\ip_\\tp$ is a vector\n",
    "    - $x^\\ip_{(\\tt),j}$ is a feature\n",
    "        - examples\n",
    "            - $\\x^\\ip_\\tp$ is a frame $\\tt$ in a movie; $x^\\ip_{(\\tt),j}$ is a pixel in frame $\\tt$\n",
    "            - $\\x^\\ip_\\tp$ are the characteristics of a stock on day $\\tt$, $x^\\ip_{(\\tt),j}$ is price or volume\n",
    "            - $\\x^\\ip_\\tp$ is a word $\\tt$ in a sentence; $x^\\ip_{(\\tt),j}$ is element $j$ of the OHE of the word"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Choices for how to predict $\\y_\\tp$ that is dependent on $\\x_{(1)} \\dots \\x_\\tp$\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\pr{\\y_\\tp | \\x_{(1)} \\dots \\x_\\tp}  & \\text{direct dependence on entire prefix } \\x_{(1)} \\dots \\x_\\tp \\\\\n",
    "\\\\\n",
    "\\pr{\\h_\\tp | x_\\tp, \\h_{(\\tt-1)} } & \\text{latent variable } \\h_\\tp \\text{encodes } \\x_{(1)} \\dots \\x_\\tp \\\\\n",
    "\\pr{\\y_\\tp | \\h_\\tp }              & \\text{prediction contingent on latent variable} \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The Recurrent Neural Network (RNN) adopts the latter approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The single layer may also emit an output at step $i$ (for outputs that are sequences).\n",
    "\n",
    "Here is some pseudo-code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "def RNN( input_sequence, state_size ):\n",
    "    state = np.random.uniform(size=state_size)\n",
    "    \n",
    "    for input in input_sequence:\n",
    "        # Consume one input, update the state\n",
    "        out, state = f(input, state)\n",
    "        \n",
    "    return out\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that RNN's are sometimes drawn without separate outputs $\\y_t$\n",
    "- in that case, $\\h_t$ may be considered the output. \n",
    "\n",
    "The computation of $\\y_\\tp$ will be just a linear transformation of $\\h_t$ so there is no loss in omitting\n",
    "it from the RNN and creating a separate node in the computation graph.\n",
    "\n",
    "Geron does not distinguish betwee $\\y_t$ and $\\h_t$ and he uses the single $\\y_\\tp$ to denote the state.\n",
    "\n",
    "I will use $\\h$ rather than $\\y$ to denote the \"hidden state\".\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## $\\h_\\tp$ latent state\n",
    "\n",
    "- $\\h$ is a *fixed length* encoding of variable length sequence $\\x_{(1)} \\dots $\n",
    "    - $\\h_\\tp$ encodes $\\x_{(1)} \\dots \\x_\\tp$\n",
    "    - gives a way to have variable length input to, e.g., classifiers\n",
    "- $\\h_\\tp$ is a vector of features\n",
    "    - captures multiple \"dimensions\"/concepts of the input sequence\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Many to one; followed by classifier</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_many_to_one_to_classifier.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Unrolling a loop\n",
    "We can \"unroll\" the loop into the sequence of steps, with time as the horizontal axis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN unrolled</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Unrolled_RNN.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that $\\x, \\y, \\h$ are all vectors. \n",
    "\n",
    "In particular, the state $\\h$ may have many elements\n",
    "-  to record information about the entire prefix of the input."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The key connection is that the state at time $t-1$ is passed as input to time $t$.\n",
    "\n",
    "So when processing $\\x_\\tp$\n",
    "- the layer can take advantage of a summary ($\\h_{(t-1)}$) of every input that preceded it.\n",
    "\n",
    "One can look at this unrolled graph as being a dynamically-created computation graph.\n",
    "\n",
    "Essentially, each state $\\h$ is replicated per time step."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This view of the computation graph is important\n",
    "- it shows you the exact computation\n",
    "- it should tell you how gradients are computed\n",
    "    - in particular, the loss and gradients flow backwards\n",
    "        - so the gradients involving $\\h$ are updated at *each* time step.\n",
    "        - this will be important when we discuss\n",
    "            - vanishing/exploding gradients\n",
    "            - skip connections\n",
    "            "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The RNN API\n",
    "\n",
    "During one time step of computation, the RNN computes 2 values\n",
    "- new  state $\\h_\\tp$\n",
    "- output $\\y_\\tp$ (sometimes simply taken to be same as shor term state\n",
    "\n",
    "The state computation is a function of the previous state $\\h_{(t-1)}$,\n",
    "and the current input $x_\\tp$.\n",
    "\n",
    "$$\n",
    "\\h_\\tp = f(\\x_\\tp;  \\h_{(t-1)})\n",
    "$$\n",
    "\n",
    "Note the recursive aspect of the computation of  $\\h_\\tp$: \n",
    "- it implicitly depends\n",
    "on the values of the states at all previous time steps $t' < t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a layer\n",
    "\n",
    "## Many to one\n",
    "\n",
    "Although the unrolled RNN looks confusing, as an \"API\" the RNN just acts as any other layer\n",
    "- takes some input $\\x$ (which happends to be a sequence)\n",
    "- produces a single output\n",
    "\n",
    "If we draw a box around the unrolled RNN, we can see the \"API\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to one</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_one.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN layer as an encoder\n",
    "The many to one RNN essentially creates a compact encoding of an arbitrarily long sequence.\n",
    "\n",
    "This can be very useful as we can feed this \"summary\" (representation) of the entire sequence\n",
    "into layers that can't handle sequence inputs.\n",
    "\n",
    "Note that there is nothing special about a layer creating a compact encoding (representation) of it's input.\n",
    "\n",
    "A CNN layer, with outputs flattened to one dimension, creates a compact encoding of an image.\n",
    "\n",
    "The real power of the RNN is the ability to encode all sequences, regardless of length, into a fixed size\n",
    "representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sequences: variable length input summarized\n",
    "\n",
    "$\\h_\\tp$ summarizes the length $\\tt$ sequence $\\x_{1,\\ldots, \\tt}$ in a *fixed size* vector $\\h_\\tp$.\n",
    "- makes sequences amenable to models that can only deal with fixed size input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be clear\n",
    "- the RNN is a layer, just like any other\n",
    "    - Internally it implements a loop but that is ordinarily hidden\n",
    "    - The intuition about the \"unrolled loop\" is to help us to better understand the inner workings, not as a coding matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Like any other layer, it produces an output (although after multiple time steps for an RNN versus\n",
    "a single time step for a Dense layer).\n",
    "                                          \n",
    "- If the length of sequence $\\x$ is $T$, there is ordinarily a **single** output $\\y_{(T)}$\n",
    "    - $\\y_{(T)}$ is only available after the entire input sequence has been consumed\n",
    "    - the intermediate results \n",
    "    $$\\h_\\tp, \\y_\\tp, \\; t = 1, \\ldots, (T-1)$$ \n",
    "    are not visible through the API\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Many to many\n",
    "\n",
    "The above behavior defines a many to one mapping from input sequence (many) to single output (one).\n",
    "\n",
    "With a minor change, we can define a many to many mapping:\n",
    "- each element of the input sequence\n",
    "results in one element of an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many Deep Learning software API's will see recurrent layers with an optional\n",
    "argument \n",
    "- `return_sequences`\n",
    "- `return_states` \n",
    "- both default to `False` in Keras.\n",
    "\n",
    "This controls the output behavior of the RNN layer, whether it returns one output per time step\n",
    "$$\n",
    "       \\h_{(1)}, \\ldots, \\h_{(T)} \\\\\n",
    "       \\y_{(1)}, \\ldots, \\y_{(T)}\n",
    "$$\n",
    "or just\n",
    "$$\n",
    "\\h_{(T)} \\\\\n",
    "\\y_{(T)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is how any RNN behaves when the function it's implementing is many to many:\n",
    "- one output per time step.\n",
    "\n",
    "When the RNN needs to implement a many to one function, the layer looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_many.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The art-work needs to be clarified\n",
    "- the RNN layer produces sequences\n",
    "    - as outputs $\\y$\n",
    "    - as states $\\h$\n",
    "\n",
    "These sequences are available when the RNN layer *completes* its consumption of input $\\x$.\n",
    "\n",
    "The following diagram may clarify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many, clarified</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop_many_many.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- the `return_sequences` argument instructs the layer to produce a sequence $\\y$\n",
    "    - rather than a scalar, as in the many to one case\n",
    "- the `return_states` argument instructs the layer to return the state $\\h$ as well\n",
    "    - useful if we stack RNN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN details: update equations\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \\\\\n",
    "\\y_\\tp & = &  \\W_{hy} \\h_\\tp  + \\b_y \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\phi$ is an activation function (usually $\\tanh$)\n",
    "\n",
    "**Note** Geron prefers right multiplying weights $\\x_\\tp \\W_{xh}$ versus $\\W_{xh}\\x_\\tp$\n",
    "- left multiplying seems more common in literature\n",
    "\n",
    "**Note**\n",
    "The equation is for a single example.  \n",
    "\n",
    "In practice, we do an entire minibatch so have $m$ $\\x's$\n",
    "given as a $(m \\times n)$ matrix $\\mathbf{X}$.\n",
    "\n",
    "**page 471: mention dimensions of each**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equation in pseudo-matrix form\n",
    "\n",
    "You will often see a short-hand form of the equation.\n",
    "\n",
    "Look at $\\h_\\tp$ as a function of two inputs $\\x_, \\h_{(t-1)}$.\n",
    "\n",
    "We can stack the two inputs into a single matrix.\n",
    "\n",
    "Stack the two matrices $\\W_{xh}, \\W_{hh}$ into a single weight matrix\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp  = \\W \\mathbf{I} + \\b \\\\\n",
    "\\text{ with } \\\\\n",
    "\\W = \\left[\n",
    " \\begin{matrix}\n",
    "    \\W_{xh} & \\W_{hh}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\mathbf{I} = \\left[\n",
    " \\begin{matrix}\n",
    "    \\x_\\tp  \\\\\n",
    "    \\h_{(t-1)}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation through time (BPTT)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>We can \"unroll\" the RNN into a sequence of layers, one per time step</li>\n",
    "        <li>In theory: Back Propagation on the unrolled RNN is the same as for a non-Recurrent Network</li>\n",
    "        <li>In practice: the unrolled RNN is very deep, which causes issues in Back Propagation.\n",
    "    </ul>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Back Propagation Through Time (BPTT)* refers to\n",
    "- unrolling the RNN computation into a sequence of layers\n",
    "- performing ordinary Back Propagation in order to update weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- In a non-Recurrent network:\n",
    "    - $\\W_\\llp$, the weights of layer $\\ll$, affect only layers $\\ll$ and greater.\n",
    "    - This means the backward flow of the gradient with respect to $\\W_\\llp$ stops at layer $\\ll$.\n",
    "- In  Recurrent Network:\n",
    "    - All unrolled \"layers\" share the *same* weights\n",
    "    - This means the gradients with respect to shared weight $\\W$ must flow backward all the way to the input layer at time $0$.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss Gradient Flow</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss_gradient.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The unrolled graph is as deep as the length of $\\x^\\ip$ ($T^\\ip = |\\x^\\ip|)$ \n",
    "- weights can update only after $T^\\ip$ input values have been processed,\n",
    "so training can be slow.\n",
    "- Vanishing Gradients become a concern for large $T^\\ip$\n",
    "    - Recall from the Vanishing Gradient lecture: magnitude of gradients diminishes from layer $\\ll$ to layer $(\\ll-1)$ during back propagation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN vanishing/exploding gradient problem\n",
    "\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>A \"single-layer RNN that has been unrolled for T time steps</li>\n",
    "        <ul>\n",
    "            <li>is mathematically equivalent to a simple NN with T layers</li>\n",
    "            <li><bold>BUT</bold> all layers share the same weights</li>\n",
    "        </ul>\n",
    "        <li>This sharing of weights leads to a problem of Vanishing/Exploding gradients</li>\n",
    "        <ul>\n",
    "            <li>Similar to the vanishing gradient problem we derived for simple NN</li>\n",
    "            <li>but with a different root cause (weight sharing)</li>\n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>Why shared weights are different</li>\n",
    "        <ul>\n",
    "            <li>Output <bold>y</bold> at time t is a function of cell state h at time t</li>\n",
    "            <li>Cell state h at time t is recursively defined</li>\n",
    "            <ul>\n",
    "                <li>So it is a function of cell states over all times t' < t as well</li>\n",
    "                <li>This means the weight update involves a repeated product: (t -t') times</li>\n",
    "                <li>This product tends to 0 (vanishing) or infinity (explode) as (t -t') increases</li>\n",
    "            </ul>\n",
    "            <li>So losses at time step t have difficulty updating gradients for the distant past<</li>\n",
    "            <li>RNN has difficulty with long-term dependencies</li>   \n",
    "        </ul>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Returning to the loss gradient\n",
    "we encountered the terms\n",
    "\n",
    "$$\\frac{\\partial \\y^\\ip_\\tp}{\\partial \\W}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will focus on the part of $\\W$ that is $\\W_{hh}$\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_\\tp}{\\partial W_{hh}} = \\frac{\\partial \\y_\\tp}{\\partial \\h_\\tp} \\frac{{\\partial \\h_\\tp}}{\\partial \\W_{hh}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But recursively defined $\\h_\\tp$ is a function of $\\h_{(\\tt-1)}, \\h_{(\\tt-1)}, \\ldots, \\h_{(1)}$\n",
    "so\n",
    "\n",
    "$$\n",
    "\\frac{\\partial \\y_\\tp}{\\partial W_{hh}} = \\frac{\\partial \\y_\\tp}{\\partial \\h_\\tp}\n",
    " \\sum_{k=0}^\\tt {  \\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt -k)}} \\frac{\\partial \\h_{(\\tt -k)}}{\\partial \\W_{hh}} }\n",
    "$$\n",
    "\n",
    "The summation: $\\frac{\\partial \\h_\\tp}{\\partial \\W_{hh}}$, through all intermediate $\\h_{(\\tt -k)}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The problematic term for us is\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt-k)}}\n",
    "$$\n",
    "\n",
    "It can be computed by the Chain Rule as\n",
    "$$\n",
    "\\frac{\\partial \\h_\\tp}{\\partial \\h_{(\\tt-k)}} = \\prod_{u=0}^{\\tt-1} { \\frac{\\partial \\h_{(\\tt -u)}}{\\partial \\h_{(\\tt - u -1) }}}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each term\n",
    "$$\n",
    "\\frac{\\partial \\h_{(\\tt -u)}}{\\partial \\h_{(\\tt - u-1)}}\n",
    "$$\n",
    "results in a term $\\W_{hh}$ so the repeated product compute matrix $\\W_{hh}$ raised to the power $k$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For simplicity,  suppose $\\W_{hh}$ were a scalar\n",
    "- if $\\W_{hh} < 1$ then repeatedly multiply $\\W_{hh}$ by itself approaches $0$\n",
    "- if $\\W_{hh} > 1$ then repeatedly multiply $\\W_{hh}$ by itself approaches $\\infty$\n",
    "\n",
    "In other words:\n",
    "- as the distance between time steps $\\tt$ and $(\\tt -k)$ increases\n",
    "- the gradient (for the weight  update) either vanishes or explodes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since this term is used in the update for our weights\n",
    "- updates  will either be erratic (too big) \n",
    "- or\n",
    "non-existent, hampering learning of weights.\n",
    "\n",
    "This was not necessarily a problem in non-recurrent networks \n",
    "- because each layer had a different\n",
    "weight matrix.\n",
    "\n",
    "What an RNN does that helps it be parsimonius in number of parameters\n",
    "- by sharing the weights across\n",
    "all time steps\n",
    "- hurts us in learning.\n",
    "                                              "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For the general case where $\\W_{hh}$ is a matrix\n",
    "- we can show the same resul with the eigenvalues of the matrix"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualization of RNN hidden state\n",
    "\n",
    "Here is a [visualization](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#visualizing-the-predictions-and-the-neuron-firings-in-the-rnn) of single elements within the hidden state, as they consume the input sequence\n",
    "of *single characters*.\n",
    "\n",
    "The color reflects the intensity (value) of the paricular cell (blue=low, red=high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>State activations after seeing prefix of input</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Unreasonable_effectiveness_1.png\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
