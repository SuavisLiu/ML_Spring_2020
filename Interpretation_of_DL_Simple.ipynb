{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "%\n",
       "%\n",
       "\n",
       "% RNN\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "%\n",
       "\n",
       "% LSTM\n",
       "\\newcommand{\\g}{\\mathbf{g}}\n",
       "\\newcommand{\\remember}{\\mathbf{remember}}\n",
       "\\newcommand{\\save}{\\mathbf{save}}\n",
       "\\newcommand{\\focus}{\\mathbf{focus}}\n",
       "%\n",
       "%\n",
       "\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "from IPython.display import Image\n",
    "\n",
    "import cnn_helper\n",
    "%aimport cnn_helper\n",
    "cnnh = cnn_helper.CNN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Basic methods for Interpretation\n",
    "\n",
    "We begin our study of Interpretability by presenting simple techniques.\n",
    "\n",
    "Our discussion will be specialized to Neural Networks\n",
    "- Consisting of multiple Convolutional Layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The reason for this specialization is two-fold\n",
    "- They are extremely common for task involving images (something that humans can easily interpret)\n",
    "- The ability of a Convolutional Layer to preserve spatial dimensions\n",
    "- Across Layers\n",
    "- Means its easy to relate features at layer $\\ll$ back to the same spatial location in the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's do a quick refresher on the important concepts and notation of Convolutional Layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# CNN refresher (notation)\n",
    "\n",
    "(We review concepts from the lecture on Convolutional Neural Networks (CNN))\n",
    "\n",
    "A *feature map* for layer $\\ll$\n",
    "- Is the value of a *single* feature at layer $\\ll$\n",
    "- At *each* spatial location\n",
    "\n",
    "An element of a feature map is the value of the feature at a single spatial location.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here are the feature maps for two layers\n",
    "- Layer $(\\ll-1)$ has three feature maps \n",
    "$$\\y_{(\\ll-1),\\ldots, k} \\text{ for features }1 \\le k \\le 3$$\n",
    "- Layer $\\llp$ has two feature maps\n",
    "$$\\y_{\\llp, \\ldots, k} \\text{ for features }1 \\le k \\le 2$$\n",
    "is the feature map for $\\y_{\\llp,1}$, feature number $1$ of layer $\\ll$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Aside: Notation reminder**\n",
    "\n",
    "The feature/channel dimension\n",
    "- Appears *last* in the subscripted list of indices (Channel Last convention)\n",
    "- The ellispes ($\\ldots$) signify the variable number of *spatial* dimensions\n",
    "- Thus feature $k$ of layer $\\ll$ is denoted $\\y_{\\llp, \\ldots, k}$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center><strong>Feature maps</strong></center>\n",
    "    <br>\n",
    "<img src=images/Conv3d_2_feature_maps.png>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each feature map $k$ of layer $\\ll$\n",
    "- Was created by applying a $(f_\\llp \\times f_\\llp \\times n_{(\\ll-1)})$ convolutional kernel $\\kernel_{\\llp,k}$\n",
    "- To layer $(\\ll-1)$ output $\\y_{(\\ll-1)}$\n",
    "\n",
    "We \"slide the kernel\" over all spatial locations  of $\\y_{(\\ll-1)}$ \n",
    "- The Convolutional Layer $\\ll$\n",
    "- Preserves the spatial dimension\n",
    "- But changes the number of features from $n_{(\\ll-1)}$ to $n_\\llp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<div>\n",
    "    <center><strong>Feature maps produced by Convolutional Layer l</strong></center>\n",
    "    <br>\n",
    "<img src=images/Conv3d_2.png>\n",
    "    </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since a Convolutional layer $\\ll$\n",
    "- Preserves the spatial dimension of its input (layer $(\\ll-1)$ output\n",
    "- Assuming full padding\n",
    "- We can directly relate the spatial location of each feature map\n",
    "- To a spatial location of layer $0$, the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The question we seek to answer:\n",
    "- Can we describe (interpret) the feature being recognized in a single feature map of layer $\\ll$ ?\n",
    "\n",
    "Much of our presentaton is based on a very influential paper\n",
    "by [Zeiler and Fergus](https://arxiv.org/abs/1311.2901)\n",
    "- NYU PhD candidate and advisor !\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Interpretation: The first layer\n",
    "\n",
    "It is relatively easy to understand the features created by the first layer\n",
    "\n",
    "Since feature map $k$ is the result of a dot-product (convolution)\n",
    "- And the dot product is performing a pattern match\n",
    "- Of the pattern given by kernel $\\kernel_{(1),k}$\n",
    "- Against a region of the input\n",
    "- We can interpret layer $1$ as trying to create synthetic features identified by the pattern\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "So all we have to do is examine each kernel to see the pattern for feature $k$ !\n",
    "\n",
    "Here is a visualization of the kernels from the Zeiler and Fergus paper\n",
    "- For 96 individual features\n",
    "- Being computed by layer $1$\n",
    "- Using a $(7 \\times 7 \\times n_{(0)})$ kernel\n",
    "\n",
    "Each square is a kernel, whose spatial dimensions are $(7 \\times 7)$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <center><strong>Layer 1 kernels</strong></center>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-004-112.jpg\", width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The \"patterns\" being recognized by these kernels seem to represent\n",
    "- Lines, in various orientations\n",
    "- Colors\n",
    "- Shading\n",
    "\n",
    "We interpret Layer $1$ as trying to construct synthetic features representing these simple concepts.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So feature map $k$ of layer $1$ can be interpretted as\n",
    "- Identifying the presence/absence of pattern $\\kernel_{(1),k}$ in input $\\x$\n",
    "- At each spatial location of the input"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Layer 1 Kernel example From  Figure 2**\n",
    "\n",
    "There are kernels looking for \"checkered\" patterns\n",
    "- At row 7, columns 1 and 5\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that examining layer $1$ kernels\n",
    "- Is *input independent*\n",
    "- Does not depend on the value of any example $\\x^\\ip$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Beyond the first layer: Clustering examples\n",
    "\n",
    "We could try to interpret the kernels of layer $(\\ll \\gt 1)$ but this will be difficult\n",
    "- Layer $\\ll$'s inputs ($\\y_{(\\ll-1)}$) are *synthetic features*, rather than actual inputs\n",
    "- Unless we understand the synthetic features of the earlier layers\n",
    "- We won't be able to interpret the pattern that layer $\\ll$ is matching"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What we can hope to do\n",
    "- Somehow map the representation created by layer $(\\ll >1)$ back to the inputs (layer 0 output)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will present several methods that\n",
    "- Are *input dependent*\n",
    "- Are conditional on the value of a particular input example $\\x^\\ip$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method will be to find *clusters* of examples\n",
    "- That produce similar feature maps\n",
    "- For map $k$ at layer $\\ll$\n",
    "\n",
    "If we can identify a property that is common to all examples in the cluster\n",
    "- We can interpret feature map $k$ of layer $\\ll$ as implementing the feature\n",
    ">\"Is the property present in the input ?\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "The first problem is that a feature map is big!\n",
    "- One element for each spatial location\n",
    "- Makes it hard to compare feature maps for similarity"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## PCA of Feature Maps\n",
    "\n",
    "It is hard to find clusters when objects are of high dimension\n",
    "- With so many dimensions\n",
    "- Any distance measurement tends to be large even for similar objects\n",
    "- Because the number of *irrelevant* elements\n",
    "- May be larger than the number of relevant elements"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider a feature map $\\y_{\\llp, \\ldots, k}$ with spatial dimension $(1000 \\times 1000)$\n",
    "- A typical image size\n",
    "- Two examples have a dog in the center\n",
    "- Surrounded by much different backgrounds\n",
    "\n",
    "If the number of spatial locations in the background is large than the region containing the dog\n",
    "- Then these two similar examples\n",
    "- Have large distance\n",
    "- Due to the different, but irrelevant, backgrounds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "We can use *dimensionality* reduction techniques of Classical Machine Learning.\n",
    "\n",
    "One such technique is Principal Components Analysis\n",
    "- Find a small number of synthetic features \n",
    "- That express commonalities of many examples\n",
    "- Represent an example in a synthetic feature space\n",
    "- Of reduced dimensions\n",
    "\n",
    "In this case: we are reducing the number of spatial locations"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here is a two layer Neural Network that we built to classify digits in the MNIST dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>MNIST CNN</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_pca_0.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We perform PCA on the representations produced by the first Convolutional Layer (dark vertical line)\n",
    "- Plotting each example \n",
    "- Using the two most important synthetic features (components) as coordinates in the plot"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>MNIST CNN Conv1 PCA</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_pca_1.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Clusters are starting to appear.\n",
    "\n",
    "Do these clusters give us a clue as to the property that the layer is representing ?\n",
    "\n",
    "- Left to right: strong vertical (\"1\", \"7\") to less vertical ?\n",
    "- Bottom to top: digits *without* \"curved tops\" to those with tops ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's perform the same analysis on the representations of the second Convolutional layer"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>MNIST CNN Conv1 PCA</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_pca_2.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The clusters become \"more pure\".\n",
    "\n",
    "So the deeper representation\n",
    "- May be finding *combinations* of input features\n",
    "- That cluster similar digits\n",
    "\n",
    "So we might be able to intepret what the first two Convolutional Layers are representing\n",
    "- Without necessarily understanding what the second layer is doing in isolation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Maximally Activating Examples\n",
    "\n",
    "The goal remains \n",
    "- Find clusters of examples\n",
    "- That produce a similar feature map $k$ at layer $\\ll$\n",
    "\n",
    "Our first attempt was to reduce the large spatial dimension of $\\y_\\llp, \\ldots, k$ to something smaller.\n",
    "\n",
    "We now try a more extreme approach."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The first difficulty we encounter for a given feature map $k$ of layer $\\ll$\n",
    "- There are many spatial locations\n",
    "\n",
    "We can reduce this complexity\n",
    "- By *summarizing* feature map $k$ with a single number\n",
    "- For example: max or average (like MaxPooling layer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This makes it easy to find examples with similar feature maps\n",
    "- They have similar summary values\n",
    "\n",
    "The method known as *Maximally Activating Examples*\n",
    "- Finds the examples in a set $\\X$\n",
    "- With the *largest* summary values\n",
    "- Recognizing \"strong\" features (max)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The method works as follows:\n",
    "- For each image in a set $\\X$\n",
    "- Compute the summary value $s^\\ip$ conditional on input $\\x^\\ip$\n",
    "- Consider all the subset of $\\X$ with the *largest absolute value* of $s^\\ip$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Here's a way to use Maximally Activating Examples to improve your Neural Network\n",
    "- We find the examples that give the strongest response\n",
    "- To the *single neuron* in Layer $L$ (the Classifier)\n",
    "- That is responsible for producing the output \"Example is an 8\"\n",
    "    - i.e., the logit that computes the score for the binary classifier \"Is 8\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center><strong>MNIST CNN maximally activating 8's</strong></center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_max_activating_8.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Interesting !  Do we have a problem with certain 8's ?\n",
    "\n",
    "Much lower probability when\n",
    "- 8 is thin versus thick\n",
    "- tilted left versus right"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Occlusion\n",
    "\n",
    "Maximally activating inputs are very coarse: they identify concepts at the level of entire input.\n",
    "    \n",
    "But, it's reasonable to suspect that some elements of the input are more important to the concept than others.\n",
    "\n",
    "In particluar, a CNN has a \"receptive field\" which defines the input elements that contribute to the layer output.\n",
    "\n",
    "Close to the input layer, the receptive field is narrow so its clear that the \"features\" being identified are small in span."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Occlusion is one way of identifying the elements of the input layer that most affect the latent\n",
    "representation.  \n",
    "\n",
    "We will describe this in terms of a 2D input, but we can generalize.\n",
    "\n",
    "Let\n",
    "- $\\y_{\\llp,j}^\\ip$ denote the response of feature $\\y_{\\llp,j}$ to input $\\x^\\ip$.\n",
    "- Place an occulding square over some portion of input $\\x^\\ip$ and measure the change in $\\y_{\\llp,j}$\n",
    "- Do this for each location in input $\\x^\\ip$ and create a \"heat map\" of changes in response $\\y_{\\llp,j}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The number on top is the percent decrease in $\\y_{(L),j}$, the logit for digit 8.\n",
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <center>Occluding 8</center>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/mnist_cnn_occlude_8.jpg\" width=800></td> \n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Not what we expected !  \n",
    "\n",
    "The mere presence of the square changes the classification probability\n",
    "greatly, even when we are not blocking the \"waist\" of the 8."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "Here is the change in response of a single feature map in layer 5 of an image classifier (Zeiler and Fergus).\n",
    "\n",
    "The chosen feature map is the one with the highest activation level in the layer.\n",
    "\n",
    "You can see that it is responding to \"faces\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Activation of one filter at layer 5</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=400\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-148.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Zeiler and Fergus also measured the change in activation of $\\y_{(L),j}^\\ip$, the logit corresponding to the correct\n",
    "class (\"Afghan Hound\")."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "      <tr>\n",
    "        <th><center>Input image</center></th>\n",
    "        <th><center>Change in logit for \"Afghan hound\"</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/img_on_page_-007-139.png\" width=400\"></td>\n",
    "        <td><img src=\"images/img_on_page_-007-145.png\" width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Conclusion\n",
    "\n",
    "We began our quest for understanding how Neural Networks work with simple techniques.\n",
    "\n",
    "The first technique\n",
    "- Find clusters of example\n",
    "- Created by a particular feature map\n",
    "- Relate a human-observable common property of the cluster\n",
    "- To the feature that the feature map is attempting to recognize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas clustering identifies groups of examples, the second technique tries to find *sub-regions* of the examples\n",
    "\n",
    "Occlusion measures the change in response of a feature map summary (or single neuron)\n",
    "- When a sub-region of the input is visible\n",
    "- Versus when it is not visible\n",
    "\n",
    "The interpretation that arises is that the feature map is attempting to recognize a property in a narrow area.\n",
    "\n",
    "So, beyond clustering, it is attempting to *localize* the spatial location of the feature."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
