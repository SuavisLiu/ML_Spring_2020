{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 1\n",
    "- Plan\n",
    "    - Motivate Machine Learning\n",
    "    - Introduce notation used throughout course\n",
    "    - Plan for initial lectures\n",
    "        - *What*: Introduce, motivate a model\n",
    "        - *How*:  How to use a model: function signature, code (API)\n",
    "        - *Why*:  Mathematical basis -- enhance understanding and ability to improve results\n",
    "        \n",
    "        \n",
    "- [Course Overview](Course_overview.ipynb)\n",
    "- [Getting Started](Getting_Started.ipynb)\n",
    "- [Machine Learning: Overview](ML_Overview.ipynb)\n",
    "- [Intro to Classical ML](Intro_Classical_ML.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 2\n",
    "- Plan\n",
    "    - Introduce a model for the Regression task: Linear Regression\n",
    "    - Introduce the Recipe for Machine Learning: detailed steps to problem solving\n",
    "- [Recap: Intro to Classical ML](Recap_of_Intro_Classical_ML.ipynb)\n",
    "\n",
    "- [Our first model: Linear Regression (Overview)](Linear_Regression_Overview.ipynb)\n",
    "- A *process* for Machine Learning\n",
    "    - Go through the methodical, multi-step process\n",
    "        - Quick first pass, followed by Deeper Dives\n",
    "    - This will be a code-heavy notebook !\n",
    "    - Illustrate Pandas, Jupyter, etc\n",
    "    - [Recipe for Machine Learning: Overview](Recipe_Overview.ipynb)\n",
    "        - [Linked notebook](Recipe_for_ML.ipynb)\n",
    "- The Loss function for Linear Regression\n",
    "    - [Linear Regression: Loss Function](Linear_Regression_Loss_Function.ipynb)\n",
    "\n",
    "- Deeper dives\n",
    "    - Iterative improvement\n",
    "        - [When to stop: Bias and Variance](Bias_and_Variance.ipynb)\n",
    "            - Regularization\n",
    "    - [Fine tuning techniques](Fine_tuning.ipynb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 3\n",
    "\n",
    "- Deferred from Week 2\n",
    "    - Prepare Data step: Introduction to Transformations\n",
    "        - Transforming data (featuring engineering) is a key step in the Recipe\n",
    "        - We introduce transformations\n",
    "            - Focus on the *how*; subsequent lecture will cover the *why*\n",
    "        - [Prepare Data: Intro to Transformations](Prepare_data_Overview.ipynb)\n",
    "        - [Transformation pipelines in `sklearn`](Transformations_Pipelines.ipynb)\n",
    "\n",
    "- Plan\n",
    "    - Introduce a model for the Classification task: Logistic Regression\n",
    "    - How to deal with Categorical (non-numeric) variables\n",
    "        - classification target\n",
    "        - features\n",
    "\n",
    "    - **Classification intro**\n",
    "        - [Classification: Overview](Classification_Overview.ipynb)\n",
    "        - [Classification and Categorical Variables](Classification_Notebook_Overview.ipynb)\n",
    "            - [linked notebook](Classification_and_Non_Numerical_Data.ipynb)\n",
    "\n",
    "    - **Non-numeric variables (categorical)**\n",
    "        - [Categorical variables](Categorical_Variables.ipynb)\n",
    "        - [Titanic using categorical features](Classification_and_Non_Numerical_Data.ipynb#Titanic-revisited:-OHE--features)\n",
    "\n",
    "    - **Classification, continued**\n",
    "        - [Multinomial Classification](Multinomial_Classification.ipynb)\n",
    "        - [Classification Loss Function](Classification_Loss_Function.ipynb)\n",
    "\n",
    "- Deeper dives\n",
    "    - [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "    - [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "    - [Log odds](Classification_Log_Odds.ipynb)\n",
    "    - [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "    \n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 2\n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 4\n",
    "\n",
    "- Deferred from Week 3\n",
    "    - Deeper dives\n",
    "        - [Baseline model for Classification](Classification_Baseline_Model.ipynb)\n",
    "        - [Dummy variable trap](Dummy_Variable_Trap.ipynb)\n",
    "        \n",
    "- Plan\n",
    "    - Error Analysis\n",
    "        - We explain Error Analysis for the Classification Task, with a detailed example\n",
    "        - How Training Loss can be improved\n",
    "    - Transformations\n",
    "        - One of the most important parts of the Recipe: transforming raw data into something that tells a story\n",
    "    - Loss functions\n",
    "        - We look at the mathematical logic behind loss functions\n",
    "\n",
    "- [Recap and Clarification: Classification](Recap_of_Classification.ipynb)\n",
    "- [Error Analysis](Error_Analysis_Overview.ipynb)\n",
    "    - [Error Analysis for Classification](Error_Analysis.ipynb)\n",
    "        - Summary statistics\n",
    "        - Conditional statistics\n",
    "        - Worked example (Deeper Dive)\n",
    "\n",
    "- [Loss Analysis: Using training loss to improve models](Training_Loss.ipynb)\n",
    " \n",
    "- [Transformations Overview](Transformations_Overview.ipynb)\n",
    "    - [Transformations: the how](Transformations_Overview.ipynb)\n",
    "    - [Transformations: the why](Transformations.ipynb)\n",
    "    \n",
    "- Deeper dives\n",
    "    - [Loss functions: the math](Loss_functions.ipynb)\n",
    "        - Maximum likelihood\n",
    "        - [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "        - Preview: custom loss functions and Deep Learning\n",
    "        \n",
    "    - [Examining errors in MNIST classification](Error_Analysis_MNIST.ipynb)\n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 2\n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 5\n",
    "\n",
    "- Plan\n",
    "    - More models: Decision Trees, Naive Bayes\n",
    "        - Different flavor: more procedural, less mathematical\n",
    "        - Decision Trees: a model with *non-linear* boundaries\n",
    "    - Ensembles\n",
    "        - Bagging and Boosting\n",
    "        - Random Forests\n",
    "    - Naive Bayes: a simple but effective model\n",
    "    \n",
    "- [Entropy, Cross Entropy, and KL Divergence](Entropy_Cross_Entropy_KL_Divergence.ipynb)\n",
    "\n",
    "- [Decision Trees: Overview](Decision_Trees_Overview.ipynb)\n",
    "- [Decision Trees](Decision_Trees_Notebook_Overview.ipynb)\n",
    "    - [linked notebook](Decision_Trees.ipynb)\n",
    "- [Trees, Forests, Ensembles](Ensembles.ipynb)   \n",
    "- [Naive Bayes](Naive_Bayes.ipynb)\n",
    "\n",
    "- Deeper Dives\n",
    "    - [Feature importance](Feature_Importance.ipynb)\n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 4\n",
    "     - [Examining errors in MNIST classification](Error_Analysis_MNIST.ipynb)\n",
    "    - Deferred from week 3  \n",
    "        - [Log odds](Classification_Log_Odds.ipynb)\n",
    "    - Deferred from Week 2 \n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Week 6\n",
    "- Plan\n",
    "    - Support Vector Classifiers: a classifier with an interesting twist\n",
    "    - Interpretation: understanding models\n",
    "    - Gradient Descent: our tools for solving optimization problems\n",
    "    \n",
    "- [Support Vector Machines: Overview](SVM_Overview.ipynb)\n",
    "- [SVC Loss function](SVM_Hinge_Loss.ipynb)\n",
    "- [SVC: Large Margin Classification](SVM_Large_Margin.ipynb)\n",
    "    \n",
    "- [SVM: Kernel Transformations](SVM_Kernel_Functions.ipynb)\n",
    "- [SVM Wrapup](SVM_Coda.ipynb)\n",
    "     \n",
    "- [Gradient Descent](Gradient_Descent.ipynb)\n",
    "\n",
    "- [Interpretation: Linear Models](Linear_Model_Interpretation.ipynb)\n",
    "  \n",
    "        \n",
    "Deeper Dives\n",
    "- [SVC Loss function derivation](SVM_Derivation.ipynb)\n",
    "- [Missing Data](Missing_Data.ipynb)\n",
    "- [Imbalanced data](Imbalanced_Data.ipynb)\n",
    "\n",
    "    \n",
    "- Deferred modules\n",
    "    - Deferred from week 4\n",
    "     - [Examining errors in MNIST classification](Error_Analysis_MNIST.ipynb)\n",
    "    - Deferred from week 3  \n",
    "        - [Log odds](Classification_Log_Odds.ipynb)\n",
    "    - Deferred from Week 2 \n",
    "        - [Fine tuning techniques](Fine_tuning.ipynb)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Assignments\n",
    "\n",
    "[Assignment Guidelines](assignments/Assignment_Guidelines.ipynb)\n",
    "\n",
    "## Assignment 1\n",
    "- [Assignment 1 notebook](assignments/Assignment_1.ipynb)\n",
    "- [Assignment 1 data](assignments/data/assignment_1)\n",
    "\n",
    "## Assignment 2\n",
    "- [Assignment 2 notebook](assignments/Assignment_2.ipynb)\n",
    "- [Assignment 2 data](assignments/data/assignment_2)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
