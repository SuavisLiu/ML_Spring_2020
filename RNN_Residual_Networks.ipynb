{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "%\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHANGES TODO**\n",
    "- All the back prop stuff has to wait until the lecture on Training\n",
    "- Can't do LSTM until backprop\n",
    "- Residual connections depend on back prop\n",
    "- Visualization OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Residual connections: a gradient highway\n",
    "[Deep Residual Learning](https://arxiv.org/abs/1512.03385)\n",
    "\n",
    "<div class=\"alert alert-block alert-warning\">\n",
    "    <b>TL;DR</b> \n",
    "    <br>\n",
    "    <ul>\n",
    "        <li>We have encountered the Vanishing Gradient problem several times.</li>\n",
    "        <li>This is a major impediment to training deep (many layers) networks.</li>\n",
    "        <li>The solution is to give the gradient a path to flow backward undiminished.</li>\n",
    "        <li>This simple solution is called a Skip or Residual Connection</li>\n",
    "    </ul>\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two layers of a NN\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{(\\ll-1)}      & = & a_{(\\ll-1)}      & \\left( f_{(\\ll-1)}( \\y_{(\\ll-2)} ) \\right) \\\\\n",
    "\\y_{\\llp} & = & a_\\llp & \\left( f_\\llp ( \\y_{(\\ll-1)} ) \\right) \\\\\n",
    "\\end{array}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we modify layer $\\ll +1$ by adding $\\y_{(\\ll-1)}$ to the  output\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{(\\ll-1)}      & = & a_{(\\ll-1)}      & \\left( f_{(\\ll-1)}( \\y_{(\\ll-2)} ) \\right) \\\\\n",
    "\\y_{\\llp} & = & a_\\llp & \\left( f'_\\llp ( \\y_{(\\ll-1)} ) \\right) + \\y_{(\\ll-1)}   \\\\\n",
    "\\end{array}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "If the original and modified 2 layer mini-networks compute the same function from $\\y_{(\\ll-1)}$ to $\\y_{(\\ll+1)}$\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "a_\\llp \\left( f_\\llp( \\y_{(\\ll-1)} ) \\right)   & = & a_\\llp  \\left( f'_\\llp ( \\y_{(\\ll-1)}) \\right)  + \\y_{(\\ll-1)} \\\\\n",
    "a_\\llp \\left( f'_\\llp ( \\y_{(\\ll-1)} ) \\right) & = & a_\\llp \\left( f_\\llp( \\y_{(\\ll-1)} ) \\right) - \\y_{(\\ll-1)}  \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "In other words: \n",
    "- we have forced the modified second layer to learn the \"residual\" of the unmodified layer with respect to $\\y_{(\\ll-1)}$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This seems strange (and pointless) until you consider the Back Propagation process.\n",
    "\n",
    "Recall how the loss gradient\n",
    "$$\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial y_\\llp}$$\n",
    "\n",
    "propagates backwards\n",
    "$$\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} \n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$\n",
    "$$\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the unmodified mini-NN  the local derivative\n",
    "$$\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} = \\frac{ a_\\llp \\left(f_\\llp(\\ldots) \\right) }{\\partial \\y_{(\\ll-1)}} \n",
    "$$\n",
    "\n",
    "whereas, in the modified mini-NN the local derivative becomes\n",
    "$$\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} = \\frac{ a_\\llp \\left( f'_\\llp (\\ldots)\\right) }{\\partial \\y_{(\\ll-1)}} + 1\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "When $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$ is multiplied by $\\loss'_\\llp$\n",
    "to obtain $\\loss'_{(\\ll-1)}$\n",
    "- the \"upstream\" loss gradient $\\loss'_{(\\ll-1)}$\n",
    "- flows backwards to layer $\\ll -1$ unmodulate *because of the* $+ 1$ term in the modified local derivative."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This simple trick vanquishes the vanishing gradient !\n",
    "\n",
    "It is one of the major reasons that we are able to train extremely deep NN's."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is another important implication:\n",
    "- adding an additional layer cannot result in increased loss\n",
    "\n",
    "This is because there exists a set of weights $\\W_\\llp$ for which \n",
    "$$\n",
    "a_\\llp \\left( f'_\\llp ( \\y_{(\\ll-1)} ) \\right) = 0\n",
    "$$\n",
    "\n",
    "This means the modified second layer computes the identity functon\n",
    "$$\n",
    "\\y_\\llp = \\y_{(\\ll-1)}\n",
    "$$\n",
    "\n",
    "So if adding the second layer had the potential of increasing loss relative to a one layer network\n",
    "- the optimization would learn the identity function instead, an no increase in loss rsults-"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Without the skip connection, it is empircally difficult for a NN to learn an identity function as a layer."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "There is an unresolved debate where to place the \"head\" of the skip connection\n",
    "- insider the activation function\n",
    "- outside the activation function\n",
    "\n",
    "We choose the latter to simplify the derivative expression for the loss gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>\"Plain\" Neural Network</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ResNet_plain.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Neural Network with skip connection</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/ResNet_skip.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Preview: skip connections in LSTM's, GRU's\n",
    "\n",
    "The gradient highway also turns out to be useful in RNN's.\n",
    "\n",
    "There are more powerful variants of the RNN called LSTM and GRU which avoid vanishing gradients, partially \n",
    "through the use of skip connections.\n",
    "\n",
    "These variants enhance the power of skip connections by allowing selective skipping via the use\n",
    "of \"gates\".\n",
    "\n",
    "We will see this shortly."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
