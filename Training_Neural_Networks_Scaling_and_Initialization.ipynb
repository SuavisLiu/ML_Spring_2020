{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{\\tp}{\\mathbf{{(t)}}}\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proper scaling of inputs\n",
    "\n",
    "## Importance of zero centered inputs (for each layer)\n",
    "[Efficient Backprop paper, LeCunn98](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "**Zero centered** means average (over the training set) value of each feature of examples is mean $0$.\n",
    "\n",
    "Gradient descent updates each element of a layer $\\ll$'s weights $\\W_\\llp$ by\n",
    "the per-example losses \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss^\\ip }{\\partial W_\\llp} & = & \\frac{\\partial \\loss^\\ip}{\\partial \\y_\\llp^\\ip} \\frac{\\partial \\y_\\llp^\\ip}{\\partial \\W_\\llp} \n",
    "\\end{array}\n",
    "$$\n",
    "summed over examples $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Over-simplifying:\n",
    "- the local derivative is proportional to the input:\n",
    "$$\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_\\llp } = a'_\\llp \\y_{(\\ll-1)}^\\ip\n",
    "$$\n",
    "for FC $y_\\llp = a_\\llp ( \\y_{(\\ll-1)} \\W_\\llp )$.\n",
    "- thus the updates of $\\W_{\\llp,j}$ will be biased by $\\bar{\\y}_{(\\ll-1),j}$ = the average (over examples $i$) of $\\y_{(\\ll-1),j}^\\ip$\n",
    "- for $\\ll = 1$, this is the average of the input feature $\\x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the particular case that each feature $j$'s average $\\bar{\\x}_j$ has the same sign:\n",
    "- updates in all dimensions will have the same sign\n",
    "- this can result in an indirect \"zig-zag\" toward the optimum\n",
    "    - Exampe: two dimensions: \n",
    "        - We can navigate the loss surface north-east or south-west only ! \n",
    "        - To get to a point north-west from the current, we have to zig-zag.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that this is an issue for *all* layers, not just layer $\\ll =1$.\n",
    "\n",
    "- Also note: the problem is compounded by activations whose outputs are not zero-centered (e.g., ReLU, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importance of unit variance inputs (weight initialization)\n",
    "\n",
    "The same argument we made for zero-centering a feature can be extended to it's variance:\n",
    "- the variance of feature $j$ over all trainng examples $i$ is the varaince of $\\y_{(\\ll-1),j}$\n",
    "\n",
    "If the variance of features $j$ and $j'$ are different, their updates will happen at different rates.\n",
    "\n",
    "We will examine this in greater depth during our discussion of weight initialization.\n",
    "\n",
    "For now: it is desirable that the input to *each* layer have it's features somewhat normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization\n",
    "\n",
    "Training is all about discovering good weights.\n",
    "\n",
    "As prosaic as it sounds: how do we *initialize* the weights before training ?\n",
    "Does it matter ?\n",
    "\n",
    "It turns out that the choice of initial weights does matter.\n",
    "\n",
    "Let's start with some *bad* choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad choices\n",
    "\n",
    "### Too big/small\n",
    "\n",
    "Layers usually consist of linear operations (e.g., matrix multiplication and addition of bias)\n",
    "followed by a non-linear activation.\n",
    "The range of many activation functions includes large regions where the derivatives are near zero,\n",
    "usually corresponding to very large/small activations.\n",
    "\n",
    "The SGD update rule uses the magnitude of the gradient to update weights.\n",
    "Obviously, if the gradients are all near-0, learning cannot occur.\n",
    "\n",
    "So one bad choice is any set of weights that tends to push activations to regions of the non-linear\n",
    "activation with zero gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identical weights\n",
    "\n",
    "Consider layer $\\ll$ with $n_\\ll$ units (neurons) implementing identical operations (e.g. FC + ReLu).\n",
    "\n",
    "Let  $\\W_{\\llp, k}$ denote the weights of unit $k$.\n",
    "\n",
    "Suppose we initialized the weights (and biases) of all units to the *same* vector.\n",
    "$$\n",
    "\\W_{\\llp, k} = \\w_\\llp, \\; 1 \\le k \\le n_\\ll\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two neuron $j, j'$ in the same layer $\\ll$\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{\\llp, j}  & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\y_{\\llp, j'} & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Both neuron will compute the same activation\n",
    "- Both neurons will have the same gradient\n",
    "- Both neurons will have the same weight update\n",
    " \n",
    "Thus, the weights in layer $i$ will start off identical and will remain identical due to identical updates.\n",
    "\n",
    "So identical initialization will lead to a failure for individual neurons to learn different features.\n",
    "\n",
    "Many approaches use some for of random initialization to break the symmetry we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glorot initialization\n",
    "\n",
    "We have previousy shown that each element $j$ of the first input layer ($\\x_{(0),j}$) should\n",
    "have unit variance across the training set.  \n",
    "\n",
    "This was meant to ensure that the first layer's weights\n",
    "updated at the same rate and that the activations of the first layer fell into regions of the activation\n",
    "function that had non-zero gradients.\n",
    "\n",
    "But this is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's assume for the moment that each element $j$ of the input vector $\\y_{(\\ll-1)}$ is mean $0$, unit variance\n",
    "and mutually independent.  \n",
    "\n",
    "So view each $\\y_{(\\ll-1),j}$ as an independent random variable with mean $0$\n",
    "and unit variance.  Furthermore, let's assome each element $\\W_{\\llp,j}$ is similarly distributed.\n",
    "\n",
    "Consider the matrix multiplication in layer $\\ll$ involving the $n_{\\ll-1}$ output of layer $\\ll$.\n",
    "$$f_\\llp(\\y_{(\\ll-1)}, W_\\llp) = \\y_{(\\ll-1)} \\cdot W_\\llp$$\n",
    "\n",
    "This expression is the weighted sum over $j$ of the product of\n",
    "- a mean 0, unit variance random variable $\\y_{(\\ll-1),j}$\n",
    "- a mean 0, unit variancerandom variable  $\\W_{\\llp,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For two random variables $X, Y$, the variance of the product \n",
    "[is](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables)\n",
    "\n",
    "$$\n",
    "\\text{Var}(XY) = \\mathbb{E}(X)^2 \\text{Var}(Y) + \\mathbb{E}(Y)^2 \\text{Var}(X) + \\text{Var}(X)\\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "So \n",
    "$$\\text{Var}(\\y_{(\\ll-1),j} \\W_{\\llp,j}) = 0^2 * 1 + 0^2 * 1 + 1 * 1 = 1\n",
    "$$\n",
    "\n",
    "Thus the variance of the dot product of $n_{\\ll-1}$ products is $n_{\\ll-1}$, not $1$ as desired.\n",
    "\n",
    "However, by scaling each $\\W_{\\llp,j}$ by \n",
    "$$\n",
    "\\frac{1}{\\sqrt{n_{\\ll-1}}}\n",
    "$$\n",
    "the variance becomes $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Glorot* (also called *Xavier*) initialization sets the initial weights to a number drawn from \n",
    "mean $0$, unit variance distribution (either normal or uniform)\n",
    "$\\frac{1}{\\sqrt{n_{\\ll-1}}}\n",
    "$.\n",
    "\n",
    "Note that we don't strictly need the requirement of unit variance -- it works equally well as long\n",
    "as the input and output variances are the same, which is the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This only partially solves the problem as it only ensures unit variance of the input to the activation function.\n",
    "\n",
    "The [original Glorot paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) justifies this\n",
    "by assuming either a $\\tanh$ or sigmoid activation functions and these functions can be approximated\n",
    "as linear in the active region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus far, we have achieved unit variance during the forward pass.\n",
    "During back propagation, it was shown that the scaling factor depends on the number of outputs\n",
    "of layer $\\ll$, rather than number of inputs, so the scaling factor needs to be\n",
    "$\\frac{1}{\\sqrt{n_\\ll}}\n",
    "$\n",
    "\n",
    "Taking the average of the two scaling factors gives a final factor of\n",
    "$\\frac{1}{\\sqrt{ \\frac{ n_{\\ll-1} + n_\\ll}{2} } } = \\sqrt{\\frac{2}{n_{\\ll-1} + n_\\ll}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaiming/He initialization\n",
    "\n",
    "[Kaiming et al](https://arxiv.org/pdf/1502.01852.pdf) extended the results of Glorot et. al\n",
    "to the ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ReLU activation has two distinct regions: one linear (for inputs greater than 0) and one all zero.\n",
    "\n",
    "The linear region of the activation corresponds to the assumption of the Glorot method.\n",
    "\n",
    "So if inputs to the ReLU are equally distributed around 0, this is approximately the same\n",
    "as the Glorot method with half the number of inputs.\n",
    "- that is: half of the ReLU's will be in the active region and half will be in the inactive region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Kaiming scaling factor is thus:\n",
    "$$\n",
    "\\sqrt{\\frac{2}{n_{(\\ll-1)}} }\n",
    "$$\n",
    "in order to preserve unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Layer-wise pre-training\n",
    "\n",
    "We discussed this in the Autoencoder lecture.\n",
    "\n",
    "In the absence of a \"good\" idea for initializing layer $\\ll$'s weights\n",
    "- train layer $\\ll$ as an Autoencdoer\n",
    "    - train it so that input $y_{(\\ll-1)}$ has target $y_{(\\ll-1)}$ \n",
    "    - this will result in weights $\\W_\\llp$ that have discovered some structure among the features $y_{(\\ll-1)}$\n",
    "    - Initialize the weights of layer $\\ll$ to the ones produced by the Autoencoder\n",
    "- These weights *may or may not* be useful in predicting $\\hat{\\y} = \\y_{(L)}$\n",
    "- But they are probably better than random weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Normalization\n",
    "\n",
    "- We addressed the importance of normalization of the inputs to layer $\\ll = 1$.\n",
    "- The same argument applies to *all* layers $\\ll > 0$\n",
    "\n",
    "We discuss some Normalization methods that attempt to keep the distribution of $\\y_{\\llp,j}$\n",
    "normalized through all layers $\\ll$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Batch normalization\n",
    "[Batch Normalization paper](https://arxiv.org/abs/1502.03167)\n",
    "\n",
    "The idea behind batch normalization:\n",
    "-  perform standardization  (mean $0$, standard deviation 1)\n",
    "at each layer, using the mean and standard deviation of each minibatch.\n",
    "\n",
    "- faciliates higher learning rate \n",
    "    - controlling the size of the derivative allows higher $\\alpha$ without increasing product\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Experimental results show that the technqiue:\n",
    "- facilitates the use of much higher learning rates, thus speeding training.  Accuracy is not lost.\n",
    "- facilitates the use of saturating activations functions (e.g., $\\tanh$ and sigmoid) which otherwise are subject to vanishing/exploding gradients.\n",
    "- acts as a regularizer; reduces the need for Dropout\n",
    "    - L2 regularization (weight decay) has *no* regularizing effect when used with Batch Normalization !\n",
    "        - [see](https://arxiv.org/abs/1706.05350)\n",
    "        - L2 regularization affects scale of weights, and thereby learning rate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Details\n",
    "\n",
    "Consider a FC layer $\\ll$ with $n_\\ll$ outputs and a mini-batch of size $m_B$.\n",
    "\n",
    "Each of the $n_\\ll$ outputs is the result of\n",
    "- passing a linear combination of $\\y_{(\\ll -1)}$ (*activation inputs*)\n",
    "-  through an activation $a_{\\llp,j}$ (*activation outputs*)\n",
    "\n",
    "We could choose to standardize either the activation inputs or the activation outputs.\n",
    "\n",
    "This algorithm standardizes the **activation inputs**.\n",
    "\n",
    "Standardization is performed relative to the mean and standard deviation of each batch.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "\n",
    "Summary for layer $\\ll$ with equation $\\y_\\llp = a_\\llp( \\W_\\llp \\y_{(\\ll-1)})$\n",
    "- each output feature $j$: $\\y_{\\llp,j} = a_{\\llp,j}( \\W_{\\llp,j} \\y_{(\\ll-1)})$\n",
    "\n",
    "- Denote the dot product for output feature $j$ by $\\x_{\\llp,j} = \\W_{\\llp,} \\y_{(\\ll-1)}$\n",
    "- We will replace $\\x_{\\llp,j}$ by a \"standardized\" $\\z_{\\llp,j}$ to be described\n",
    "\n",
    "Rather than carrying along supscript $j$\n",
    "we write all operations on  the collection $\\x_{\\llp,j}$ as a vector operation on $\\x_\\llp$ for ease of notation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "$\n",
    "\\begin{split}\n",
    "1.\\quad & \\mathbf{\\mu}_B = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{\\mathbf{x}^{(i)}}\\\\\n",
    "2.\\quad & {\\mathbf{\\sigma}_B}^2 = \\dfrac{1}{m_B}\\sum\\limits_{i=1}^{m_B}{(\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B)^2}\\\\\n",
    "3.\\quad & \\hat{\\mathbf{x}}^{(i)} = \\dfrac{\\mathbf{x}^{(i)} - \\mathbf{\\mu}_B}{\\sqrt{{\\mathbf{\\sigma}_B}^2 + \\epsilon}}\\\\\n",
    "4.\\quad & \\mathbf{z}^{(i)} = \\gamma \\hat{\\mathbf{x}}^{(i)} + \\beta\n",
    "\\end{split}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So\n",
    "- $\\mathbf{\\mu}_B, \\mathbf{\\sigma}_B$ are vectors (of length $n_\\ll$) of \n",
    "    - the element-wise means and standard deviations (computed across the batch of $m_B$ examples)\n",
    "- $\\mathbf{\\hat{x}^{(i)}}$ is standardized $\\mathbf{x}^{(i)}$ \n",
    "\n",
    "** Note the $\\epsilon$ in the denominator is their solely to prevent \"divide by 0\" errors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "What is going on with $\\z^\\ip$ ?  \n",
    "\n",
    "Why are we constructing it with mean $\\beta$ and standard deviation $\\gamma$ ?\n",
    "\n",
    "$\\beta, \\gamma$ which are **learned** parameters.\n",
    "\n",
    "Why should $\\beta, \\gamma$ be learned ?\n",
    "\n",
    "At a minimum: it can't hurt:\n",
    "- it admits the possibility of the identity transformation\n",
    "    - which would be the simple standardization\n",
    "- but allows the unit to be non-linear when there is a benefit"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Moreover, depending on the activation $a_{\\llp, j}$\n",
    "- $\\hat{\\x}_{\\llp,j}$ can wind up *within the active region* of the activation function\n",
    "\n",
    "This effectively makes our transformations linear, rather than non-linear, which are more powerful.\n",
    "\n",
    "By shifting the mean by $\\beta$ we gain the *option* to avoid this should it be beneficial.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The final question is: what do we do at inference/test time, when all \"batches\" are of size 1 ?\n",
    "\n",
    "The answer is\n",
    "- compute a single $\\mathbf{\\mu}, \\mathbf{\\sigma}$ from the sequence of such values across all batches.\n",
    "- \"population\" statistics (over full training set\n",
    "- rather than \"sample\" statistics (from a single training batch).\n",
    "\n",
    "Typically a moving average is used.\n",
    "We refer readers to the paper."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We create a new layer type $\\text{BN}$ to perform Batch Normalization to the inputs of any layer.\n",
    "\n",
    "Thus, it particpates in both the forward (i.e., normalization) and backward (gradient computation)\n",
    "steps."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Unbelievably good initialization\n",
    "\n",
    "Glorot and Kaiming weight initialization \n",
    "- ensures \"good\" distribution of outputs of a layer, given a good distribution of inputs to the layer\n",
    "\n",
    "Normalization (e.g., Batch Normalization)\n",
    "- tries to ensure good distribution of inputs across al layers\n",
    "\n",
    "There are some initialization methods that attempt to create weights that are so good,\n",
    "that Normalization during training is no longer necessary.\n",
    "\n",
    "[Fixup initialization paper](https://arxiv.org/abs/1901.09321)\n",
    "- good initialization means you don't need normalization layers\n",
    "\n",
    "But good initialization can help too.\n",
    "  \n",
    "    \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Learning rate schedules\n",
    "\n",
    "In addition to smarter optimzers, we can control learning rates by changing them across epochs of\n",
    "training.\n",
    "\n",
    "This is very much of an art rather than a science.\n",
    "\n",
    "We give a brief overview."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Warm up\n",
    "[Bag of Tricks for Image Classification using CNNs](https://arxiv.org/abs/1812.01187)\n",
    "\n",
    "- When training starts: initial values of $\\W$ far optimal values\n",
    "- At this point, losses (and gradients) are probably large\n",
    "    - large updates to $\\W$ might cause instability\n",
    "\n",
    "So, we can start off \"slow\" with a low initial rate during a *warm-up period*.\n",
    "- low learning rate compensates for high gradient\n",
    "\n",
    "Post the warm-up, we can use a higher rate to speed training."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Post warm-up\n",
    "\n",
    "Typical strategy has been to decrease learning rate as the number of epochs increase.\n",
    "\n",
    "Idea is to take smaller steps as we approach the region of optimality\n",
    "- don't want to overshoot\n",
    "\n",
    "There are many ways to set a learning rate schedule (a function that maps epoch number to a rate)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- step schedule\n",
    "    - vary rate by epoch\n",
    "    - rate decreases as epoch increases\n",
    "- cosine decay\n",
    "    - decrease rate according to a cosine function\n",
    "        - $\\text{learning_rate}_{t} = {1\\over{2}} \\left( 1 + \\cos(  \\pi{{t}\\over{T}} ) \\right)  * \\text{learning_rate}_{0} $\n",
    "            - where $\\text{learning_rate}_{0}$ is initial learning rate, $T$ is number of batches\n",
    "        - slow decrease in rate at start\n",
    "        - near-linear decrease in middle\n",
    "        - slow decrease near end"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 720x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "_= nnh.plot_cosine_lr()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Regularization\n",
    "\n",
    "The ultimate goal of Machine Learning is out of sample prediction.\n",
    "\n",
    "Because Neural Networks often learn a large number of parameters (weights), overfitting is a concern.\n",
    "\n",
    "We will briefly review several methods to combat overfitting"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Cost function: add regularization penalty\n",
    "\n",
    "The same methods that were applicable in Classical Machine Learning apply to Deep Learning as well.\n",
    "\n",
    "These include regularization penalties that aim to reduce the number of parameters.\n",
    "- L2 regularization\n",
    "- L1 regularization"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Dropout\n",
    "\n",
    "[Droput paper](http://www.jmlr.org/papers/volume15/srivastava14a/srivastava14a.pdf)\n",
    "\n",
    "Overfitting can occur because some weights in the NN adapt so as to memorize \"noisy\" features.\n",
    "\n",
    "*Dropout* is a method that *randomly drops a unit in the NN*\n",
    "- For each training example $\\x^\\ip$\n",
    "- Each unit gets dropped with probability $p$\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>NN, no dropout</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=images/Dropout_NN_wo_dropout.jpg width=400></td>\n",
    "    </tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>NN, 50% dropout</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=images/Dropout_NN_w_dropout.jpg width=400></td>\n",
    "    </tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A Neural Network with $N$ units contains $2^N$ possible sub-networks.\n",
    "\n",
    "Dropout can be viewed as training many of these sub-networks (with weights shared by sub-networks.)\n",
    "\n",
    "If a feature is truly important, the NN must adapt to robustly recognize the feature.\n",
    "\n",
    "If it is not important, the goal is to prevent a unit from memorizing it."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In Keras, Dropout is implemented by a layer:\n",
    "\n",
    "`Dropout(rate)`\n",
    "\n",
    "where `rate` is the probability of dropping a unit.\n",
    "\n",
    "Dropout has been supplanted by Batch Normalization, but is worth studying \n",
    "- for its simplicity and ease of use\n",
    "- inspiration it offers.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Data Augmentation \n",
    "\n",
    "It is sometimes possible to expand the training set in such a way as to discourage overfitting.\n",
    "\n",
    "This usually involves \n",
    "creating variants of training examples\n",
    "- make it hard to memorize them all."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Input Transformation\n",
    "\n",
    "Alter the image while preserving its label.\n",
    "\n",
    "- Image transformation\n",
    "    - rotate, crop, flip"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Label smoothing\" reducing prediction confidence\n",
    "\n",
    "[Label Smoothing paper](https://arxiv.org/pdf/1701.06548.pdf)\n",
    "\n",
    "Recall our discussion about the difference between Cross Entropy and Hinge Loss\n",
    "- Hinge Loss \"stops trying\" to improve parameters when they are just \"good enough\" to yield a correct prediction\n",
    "- Cross Entropy: tries to improve probablity to exactly $0$ or $1$\n",
    "\n",
    "Cross Entropy's relentless search for improvement may lead to poor out of sample generalization."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "A class of solutions exists to discourage the NN from seeking absolute confidence in its prediction.\n",
    "\n",
    "*Label Smoothing* changes binary targets to values that are only approximately $0$ or $1$.\n",
    "\n",
    "| Example | Smoothed label\n",
    "| :- | -----\n",
    "| $(\\x^\\ip, 0)$ | $(\\x^\\ip, 0 + \\epsilon)$\n",
    "| $(\\x^\\ip, 1)$ | $(\\x^\\ip, 1 - \\epsilon)$\n",
    "\n",
    "\n",
    "So rather than using One Hot Encoding (OHE), we use \"$\\epsilon$ Hot Encoding\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Mixup training\n",
    "[Mixup training paper](https://arxiv.org/abs/1905.11001)\n",
    "\n",
    "*Mixup training* is a second solution to prevent an NN from seeking absolute confidence.\n",
    "\n",
    "It creates additional training examples that are *mixtures* of existing examples:\n",
    "\n",
    "\n",
    "| &nbsp; &nbsp; &nbsp; &nbsp; Training example &nbsp; &nbsp; &nbsp; &nbsp; &nbsp;         | Mixup ? \n",
    "|:--- | :--- \n",
    "| $(\\x^\\ip, \\y^\\ip)$ &nbsp; &nbsp; &nbsp;  &nbsp; | original\n",
    "| $(\\x^{(i')}, \\y^{(i')})$ &nbsp; &nbsp; &nbsp; | original\n",
    "| $(\\x^\\ip + \\lambda \\x^{(i')}, \\y^\\ip) + \\lambda \\y^{(i')})$ | Mixup\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The mixing parameter $\\lambda$ is best when it is close to $0$ or $1$\n",
    "- $(0 + \\epsilon)$ or $(1 - \\epsilon)$."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
