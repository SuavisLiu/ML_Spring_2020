{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "% CNN\n",
       "\\newcommand{\\kernel}{\\mathbf{k}} \n",
       "\\newcommand{\\dim}{d}\n",
       "%\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**CHANGES TODO**\n",
    "- All the back prop stuff has to wait until the lecture on Training\n",
    "- Can't do LSTM until backprop\n",
    "- Residual connections depend on back prop\n",
    "- Visualization OK"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The RNN API\n",
    "\n",
    "During one time step of computation, the RNN computes 2 values\n",
    "- new  state $\\h_\\tp$\n",
    "- output $\\y_\\tp$ (sometimes simply taken to be same as shor term state\n",
    "\n",
    "The state computation is a function of the previous state $\\h_{(t-1)}$,\n",
    "and the current input $x_\\tp$.\n",
    "\n",
    "$$\n",
    "\\h_\\tp = f(\\x_\\tp;  \\h_{(t-1)})\n",
    "$$\n",
    "\n",
    "Note the recursive aspect of the computation of  $\\h_\\tp$: \n",
    "- it implicitly depends\n",
    "on the values of the states at all previous time steps $t' < t$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a layer\n",
    "\n",
    "## Many to one\n",
    "\n",
    "Although the unrolled RNN looks confusing, as an \"API\" the RNN just acts as any other layer\n",
    "- takes some input $\\x$ (which happends to be a sequence)\n",
    "- produces a single output\n",
    "\n",
    "If we draw a box around the unrolled RNN, we can see the \"API\":"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to one</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_one.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## RNN layer as an encoder\n",
    "The many to one RNN essentially creates a compact encoding of an arbitrarily long sequence.\n",
    "\n",
    "This can be very useful as we can feed this \"summary\" (representation) of the entire sequence\n",
    "into layers that can't handle sequence inputs.\n",
    "\n",
    "Note that there is nothing special about a layer creating a compact encoding (representation) of it's input.\n",
    "\n",
    "A CNN layer, with outputs flattened to one dimension, creates a compact encoding of an image.\n",
    "\n",
    "The real power of the RNN is the ability to encode all sequences, regardless of length, into a fixed size\n",
    "representation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sequences: variable length input summarized\n",
    "\n",
    "$\\h_\\tp$ summarizes the length $\\tt$ sequence $\\x_{1,\\ldots, \\tt}$ in a *fixed size* vector $\\h_\\tp$.\n",
    "- makes sequences amenable to models that can only deal with fixed size input\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be clear\n",
    "- the RNN is a layer, just like any other\n",
    "    - Internally it implements a loop but that is ordinarily hidden\n",
    "    - The intuition about the \"unrolled loop\" is to help us to better understand the inner workings, not as a coding matter"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Like any other layer, it produces an output (although after multiple time steps for an RNN versus\n",
    "a single time step for a Dense layer).\n",
    "                                          \n",
    "- If the length of sequence $\\x$ is $T$, there is ordinarily a **single** output $\\y_{(T)}$\n",
    "    - $\\y_{(T)}$ is only available after the entire input sequence has been consumed\n",
    "    - the intermediate results \n",
    "    $$\\h_\\tp, \\y_\\tp, \\; t = 1, \\ldots, (T-1)$$ \n",
    "    are not visible through the API\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Many to many\n",
    "\n",
    "The above behavior defines a many to one mapping from input sequence (many) to single output (one).\n",
    "\n",
    "With a minor change, we can define a many to many mapping:\n",
    "- each element of the input sequence\n",
    "results in one element of an output sequence."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Many Deep Learning software API's will see recurrent layers with an optional\n",
    "argument \n",
    "- `return_sequences`\n",
    "- `return_states` \n",
    "- both default to `False` in Keras.\n",
    "\n",
    "This controls the output behavior of the RNN layer, whether it returns one output per time step\n",
    "$$\n",
    "       \\h_{(1)}, \\ldots, \\h_{(T)} \\\\\n",
    "       \\y_{(1)}, \\ldots, \\y_{(T)}\n",
    "$$\n",
    "or just\n",
    "$$\n",
    "\\h_{(T)} \\\\\n",
    "\\y_{(T)}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is how any RNN behaves when the function it's implementing is many to many:\n",
    "- one output per time step.\n",
    "\n",
    "When the RNN needs to implement a many to one function, the layer looks like"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": false,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_many_many.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The art-work needs to be clarified\n",
    "- the RNN layer produces sequences\n",
    "    - as outputs $\\y$\n",
    "    - as states $\\h$\n",
    "\n",
    "These sequences are available when the RNN layer *completes* its consumption of input $\\x$.\n",
    "\n",
    "The following diagram may clarify\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN many to many, clarified</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_loop_many_many.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- the `return_sequences` argument instructs the layer to produce a sequence $\\y$\n",
    "    - rather than a scalar, as in the many to one case\n",
    "- the `return_states` argument instructs the layer to return the state $\\h$ as well\n",
    "    - useful if we stack RNN layers"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Stacked RNN layers\n",
    "\n",
    "One can connect RNN layers into \"stacks\" \n",
    "- by feeding\n",
    "the output state of one RNN layer as the input to the successor layer:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Stacked layers</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layers_stacked.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "​"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Encoder/Decoder architecture\n",
    "\n",
    "An *Encoder/Decoder* is a two part Neural Network that is applied to many NLP tasks\n",
    "- *Encoder* converts sequence (sentence) into intermediate representation (sequence)\n",
    "- *Decoder* converts intermediate sequence to final sequence"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "**IS THE DIAGRAM CORRECT ??**\n",
    "\n",
    "Or is it\n",
    "- Encoder: many to one\n",
    "- Decoder: one to many\n",
    "\n",
    "Attention\n",
    "- Encoder: many to many\n",
    "- Decoder: many to many"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Encoder/Decoder</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_Encoder_Decoder.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>\n",
    "​\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Sequence to Sequence\n",
    "\n",
    "- Many to one encoder\n",
    "    - variable length input to fixed length final state $\\h$\n",
    "- One to one decoder\n",
    "    - *initial* state of decoder set to *final* state of encoder\n",
    "    - teacher forcing\n",
    "        - input $(\\tt +1)$ of decoder is out $\\tt$ of decoder\n",
    "\n",
    "Example: language translation\n",
    "\n",
    "This is useful when there is not an exact one to one correspondence between tokens in the source and target languages."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: inference</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq_inference.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Sequence to Sequence: training (teacher forcing)</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_seq2seq_training.jpg\" width=1000></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN details: update equations\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h_{(t-1)}  + \\b_h) \\\\\n",
    "\\y_\\tp & = &  \\W_{hy} \\h_\\tp  + \\b_y \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where $\\phi$ is an activation function (usually $\\tanh$)\n",
    "\n",
    "**Note** Geron prefers right multiplying weights $\\x_\\tp \\W_{xh}$ versus $\\W_{xh}\\x_\\tp$\n",
    "- left multiplying seems more common in literature\n",
    "\n",
    "**Note**\n",
    "The equation is for a single example.  \n",
    "\n",
    "In practice, we do an entire minibatch so have $m$ $\\x's$\n",
    "given as a $(m \\times n)$ matrix $\\mathbf{X}$.\n",
    "\n",
    "**page 471: mention dimensions of each**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Equation in pseudo-matrix form\n",
    "\n",
    "You will often see a short-hand form of the equation.\n",
    "\n",
    "Look at $\\h_\\tp$ as a function of two inputs $\\x_, \\h_{(t-1)}$.\n",
    "\n",
    "We can stack the two inputs into a single matrix.\n",
    "\n",
    "Stack the two matrices $\\W_{xh}, \\W_{hh}$ into a single weight matrix\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h_\\tp  = \\W \\mathbf{I} + \\b \\\\\n",
    "\\text{ with } \\\\\n",
    "\\W = \\left[\n",
    " \\begin{matrix}\n",
    "    \\W_{xh} & \\W_{hh}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\mathbf{I} = \\left[\n",
    " \\begin{matrix}\n",
    "    \\x_\\tp  \\\\\n",
    "    \\h_{(t-1)}\n",
    " \\end{matrix} \n",
    " \\right] \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Stacked RNN layers revisited\n",
    "\n",
    "With the benefit of the RNN update equations, we can clarify how stack RNN layers works>\n",
    "\n",
    "Let superscript $[\\ll]$ denote a stacked layer of RNN.\n",
    "\n",
    "So the RNN update equation for the bottom layer $1$ becomes\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h^{[1]}_\\tp & = & \\phi(\\W_{xh}\\x_\\tp  + \\W_{hh}\\h^{[1]}_{(t-1)}  + \\b_h) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "The RNN update equation for leyer $[\\ll]$ becomes\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\h^{[\\ll]}_\\tp & = & \\phi(\\W_{xh}\\h^{[\\ll-1]}_\\tp  + \\W_{hh}\\h^{[\\ll]}_{(t-1)}  + \\b_h) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is: the input to layer $[\\ll]$ is $\\h^{[\\ll-1]}_\\tp$ rather than $\\x_\\tp$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Loss function\n",
    "\n",
    "Examples\n",
    "- an example $\\x^\\ip$ is now a *sequence* $\\x^\\ip_{(1)}, \\x^\\ip_{(2)}, \\ldots, \\x^\\ip_{(T)} $\n",
    "    - variable length\n",
    "    - $\\x^\\ip_\\tp$ *may* be a vector (doesn't have to be scalar), \n",
    "        - e.g., word embedding\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Per example loss $\\loss^\\ip$ *per time step*\n",
    "- In many to many: there is a loss per time-step.  \n",
    "- Total loss (over which we optimize) is sum, orver time , of the loss per time step\n",
    "    - $\\loss^\\ip = \\sum_{\\tt=1}^n \\loss^\\ip_\\tp$\n",
    "- In many to one: loss is single value (per example): depends on final state \n",
    "    - $\\loss^\\ip = \\loss_{(T)}$ "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>RNN Loss: Forward pass</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_layer_loss.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Sequences: Variable length\n",
    "\n",
    "There are lots of small potholes one encounters with sequences.\n",
    "\n",
    "What is the examples of my training set have widely varying lengths ?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Within a batch, short examples may behave differently than long examples:\n",
    "    - Maybe learn less in short examples, noisier gradient updates\n",
    "    \n",
    "- Padding sequences to make them equal length\n",
    "    - Pad at the start ? Or at the end ?\n",
    "\n",
    "The general advice is to arrange your data so that an epoch contains examples of similar lengths.\n",
    "- You may require multiple fittings, one per length"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Visualization of RNN hidden state\n",
    "\n",
    "Here is a [visualization](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#visualizing-the-predictions-and-the-neuron-firings-in-the-rnn) of single elements within the hidden state, as they consume the input sequence\n",
    "of *single characters*.\n",
    "\n",
    "The color reflects the intensity (value) of the paricular cell (blue=low, red=high)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>State activations after seeing prefix of input</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/Unreasonable_effectiveness_1.png\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues with RNN's\n",
    "\n",
    "Preview:\n",
    "- back prop, BPTT\n",
    "- exploding gradient\n",
    "- vanishing gradient\n",
    "\n",
    "- Length of a sequence potentially very long\n",
    "    - Gradient computation\n",
    "    - Can we/should we really unroll the whole loop\n",
    "    - Vanishing/exploding gradients\n",
    "    - Dealing with long sequences in Keras\n",
    "\n",
    "These are special case of the issue of Back Prop.\n",
    "\n",
    "Will defer many issues until after we learn Back Prop"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# RNN as a generative model\n",
    "\n",
    "\n",
    "Up to now, an RNN's inputs were a prespecificed vector $\\x$.\n",
    "\n",
    "For each example during training, one element of $\\x$ was fed into the RNN per time-step.\n",
    "\n",
    "Similarly for inference time.\n",
    "\n",
    "This behavior is characteristic of a discriminative network."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider: Suppose there were **no** inputs (or more precisely: a very short sequence $\\x$ of length $t'$, used to \"prime\" the RNN).\n",
    "\n",
    "Instead, let's set the input at time step $t$ to be the output of step $(t-1)$\n",
    "$$\n",
    "\\x_\\tp = \\y_{(t-1)}\n",
    "$$\n",
    "for $t > t'.\n",
    "\n",
    "Then the RNN would be self-perpetuating, never exhausting its inputs, and generating new outputs\n",
    "*conditional* on previous outputs !\n",
    "\n",
    "This would be a generative form of the RNN."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Training by teacher forcing\n",
    "\n",
    "One way to train this RNN is via a supervised task\n",
    "- given sequence $\\x$ up to time $t$: $\\x_{(1, \\ldots t)}$\n",
    "- target is $\\x_{(t+1)}$\n",
    "\n",
    "This is just ordinary supervised training with a specially constructed input derived from a single sequence $\\x$.  \n",
    "\n",
    "Of course, we would do this for a training set with many sequences, as usual."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is similar to a classifier where the class we are trying to predict is the class of the next input\n",
    "element.\n",
    "\n",
    "The only \"trick\" is that, at step $t$, the RNN may output the \"wrong\" value $\\hat{\\x} \\ne \\x_{(t+1)}$.\n",
    "If we fed the wrong $\\hat{\\x}$ as the next input to the RNN during training, the RNN would remain\n",
    "permanently off-track and never learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Instead, during *training*, the next input to the RNN \n",
    "- is **forced** to be the correct input (**is the input at step $t$ \n",
    "\n",
    "$$\n",
    "\\x_{(t+1)} \\text{ or is it } \\x_{(t+1)}\n",
    "$$\n",
    "\n",
    "This type of supervised learning is called *teacher forcing*.\n",
    "\n",
    "During *inference* time, we feed back as input whatever the generated output is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Sampling\n",
    "We have described a deterministic generation process.\n",
    "\n",
    "This would be pretty boring, lacking variety\n",
    "- as well as being problematic for generalization\n",
    "- we would be encouraging\n",
    "the RNN to memorize inputs.\n",
    "\n",
    "In producing the single output, what is really happening is\n",
    "- our classifier has one logit per\n",
    "class\n",
    "- we arbitarily decide to pick the largest."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "But we can properly view the (post-softmax) output\n",
    "- as a probability vector (elements sum to $1$).\n",
    "\n",
    "Instead of choosing the class with maximum probability\n",
    "- we can sample from the probability space\n",
    "defined by this vector\n",
    "- e.g., if the probability for class $c_1$ is twice as great as that for class\n",
    "$c_2$, the probability of sampling $c_1$ would be twice as great.  \n",
    "\n",
    "There is still some chance\n",
    "that $c_2$is sampled, unlike the deterministic case.\n",
    "\n",
    "If we do this, the generator can create output sequences different from any training example."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Training, with Teacher Forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_teacher_forcing_training.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Test time: no forcing</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/RNN_teacher_forcing_inference.jpg\" width=800\"></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Generating strange things\n",
    "\n",
    "We haven't specified what each element in sequence $\\x$ is.\n",
    "\n",
    "For text, $x_\\tp$ could be either a character or a word, for example.\n",
    "\n",
    "You'll be surprised how successful an RNN can be when it's task is to consume sequences of characters\n",
    "and predict the next character.\n",
    "\n",
    "Although it hasn't been expicitly programmed to generate valid words, punctuation, etc.,\n",
    "it tends to produce realistic text !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Another interesting fact: these \"character RNN's\" also learn semantically meaningful constructs\n",
    "- the need for nested things to match\n",
    "    - multi-level paranthetical phrases, e.g., \"(this is (very important) I think)\"\n",
    "    - opening/closing markup\n",
    "    - indentation/un-indentation of code blocks\n",
    "\n",
    "This suggests that the hidden state may be learning to \"count\" certain concepts.\n",
    "\n",
    "As we will see in a visualization of the hidden state, and in how LSTM's work, this may in fact be true."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "RNN's of this type were quite popular and have been used to generate\n",
    "- Fake [Shakespeare](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#shakespeare), or fake politician-speak\n",
    "- Fake code \n",
    "- Fake [math textbooks](http://karpathy.github.io/2015/05/21/rnn-effectiveness/#algebraic-geometry-latex)\n",
    "- [Click bait headline generator](http://clickotron.com/about)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
