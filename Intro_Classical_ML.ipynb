{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Macro `_latex_std_` created. To execute, type its name (without quotes).\n",
      "=== Macro contents: ===\n",
      "get_ipython().run_line_magic('run', 'Latex_macros.ipynb')\n",
      " "
     ]
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Overview\n",
    "\n",
    "This is the \"trailer\" for the course: a brief plot summary and introduction to the key characters\n",
    "you will encounter.\n",
    "\n",
    "## Goals\n",
    "- Get a high level view of Machine Learning\n",
    "- Introduce notation\n",
    "- Preview concepts\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How this course is different\n",
    "\n",
    "Our belief is that Machine Learning should be taught as a *process* for problem solving.\n",
    "\n",
    "The following picture will be our agenda; each column is one step in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<img src=images/ML_process.jpg>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In contrast, many approaches focus on a few steps under \"Train a model\"\n",
    "- Select a model\n",
    "- Fit\n",
    "\n",
    "At two extremes, these approaches either focus on \"using an API\" or deep math.\n",
    "\n",
    "This may lead to the ability to construct models but, in our opinion, what distinguishes an adequate Data Scientist from a good one are all the other steps in the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To be sure, this course will both teach you how to use an API for Machine Learning and contain a fair amount of math.\n",
    "\n",
    "But we take an engineer/scientist approach and focus on insight and repeatability (hence, process)\n",
    "- we view Data Science as an experimental science\n",
    "- your experiments are implemented via code\n",
    "- you need to understand enough math to diagnose problems and improve experiments\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "So *expect to do a lot of coding*.\n",
    "- You don't need to be a \"professional\" programmer\n",
    "- But you do need to be a *disciplined* programmer to ease repeatability\n",
    "    - Subprograms/classes (methods) versus cut and paste\n",
    "    \n",
    "Also expect some math\n",
    "- in order to understand why a model is appropriate or not, and to diagnose why it is not working\n",
    "- **not** to be able to derive formulae"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Classical ML and Deep Learning\n",
    "\n",
    "There are two main streams in this course\n",
    "- \"Classical ML\"\n",
    "    - somewhat long history\n",
    "    - somewhat related to Statistics\n",
    "- \"Deep Learning\"\n",
    "    - really took off after 2010\n",
    "    - more related to Artificial Intelligence than Statistics\n",
    "        - experimental versus mathematical\n",
    "        \n",
    "This preview is for Classical Machine Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The big picture\n",
    "<img src=external/scipy-2018-sklearn/notebooks/figures/ml_taxonomy.png>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Supervised Learning\n",
    "\n",
    "Supervised learning is about *informed prediction*.\n",
    "\n",
    "Let's parse these word\n",
    "- prediction\n",
    "- informed"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>Prediction: what digits do these pixels represent ?</center>\n",
    "    </tr>\n",
    "<img src=images/mnist_small_test.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Prediction**:\n",
    "Given an image that we haven't seem before, determine which digit it represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "More formally:\n",
    "- A single input $\\x$ is a vector of length $n$, i.e., a collection of $n$ *features*.\n",
    "\n",
    "- A **predictor** is a map from $\\x$ to a class (label) $\\hat{\\y}$.\n",
    "\n",
    "The previously unseen image: $\\x$ consists of $n=64$ pixels (arranged as an $8 \\times 8$ grid.\n",
    "\n",
    "$\\hat{\\y}$ is the digit that we will say the image represents."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For now: \n",
    "- a class is drawn from a finite set $C$ of potential classes.\n",
    "- we are describing Classification -- mapping $\\x$ to a single class.\n",
    "- we will extend to Regression: outputs are from a continous universe (e.g., numbers)\n",
    "\n",
    "An example is a pair $(\\x, c)$ of a feature vector $\\x$ and a class $c \\in C$ (the target)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Informed prediction** is when the probability of the predictor making a correct prediction\n",
    "is greater than $\\frac{1}{||C||}$.\n",
    "\n",
    "- Consider a single example $(\\x,c)$.\n",
    "\n",
    "- A simple but naive predictor would map $\\x$ to a random $c' \\in C$.\n",
    "\n",
    "The probability of the predictor being correct ($c' = c$) is\n",
    "$\\frac{1}{||C||}$ .\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "DnCAQKeGFAo0",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "How do we achieve this ?\n",
    "\n",
    "*Supervised Learning* makes the prediction based on having seen multiple, correctly labelled examples.\n",
    "- It tries to *generalize*: find some pattern in the examples that is associated with the label.\n",
    "- Perhaps the individual features (elements of $\\x$) are associated with the correct class $c$.\n",
    "\n",
    "- The aim of Supervised Learning is to create a function (predictor) that maps an $\\x$ to the correct $c$.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNAynOcdBplo",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Notation\n",
    "\n",
    "- Supervised Learning involves supplying a number ($m$) of examples.\n",
    "- Each example is a pair consisting of\n",
    "    - vector $\\x$ consisting of $n \\ge 1$ *features* (attributes)\n",
    "    - scalar (sometimes a vector) $\\y$\n",
    "        - refered to as the*target* value or *label* associated with $\\x$\n",
    "\n",
    "- we use **bold face** to indicate a vector (e.g, $\\x$)\n",
    "- We use superscript $\\ip$ to denote an element $i$ of a collection of $m$ examples (e.g., $\\x^\\ip$)\n",
    "- We use subscript $j$ to index element $j$ of a vector, e.g., $\\x^\\ip_j$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "UNAynOcdBplo",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- So $\\x^\\ip$ is\n",
    "\n",
    "$\n",
    "  \\x^\\ip = \\begin{pmatrix}\n",
    " \\x^\\ip_1 \\\\\n",
    " \\x^\\ip_2 \\\\\n",
    "  \\vdots  \\\\\n",
    " \\x^\\ip_n\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "Each  element of $\\x^\\ip$ is a \"feature\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Not just numbers !\n",
    "\n",
    "The features *aren't restricted to be numeric* !\n",
    "\n",
    "In this course, we will deal with data that is\n",
    "- numeric\n",
    "- categorical\n",
    "- text\n",
    "- image\n",
    "- sound (not this course)\n",
    "\n",
    "Of course, you'll have to encode this data as numbers in order for numerical algorithms to handle them."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1441BMaCos9",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Training set\n",
    "\n",
    "- The collection of examples used for fitting (training) a model is called the *training set*:\n",
    "\n",
    "$$ \\langle \\X, \\y \\rangle= [ \\x^\\ip, \\y^\\ip | 1 \\le i \\le m ]$$\n",
    "\n",
    "where $m$ is the size of training set and each $\\x^\\ip$ is a feature vector of length $n$.\n",
    "\n",
    "- By seeing many ($m$) pairs of feature vectors and associated labels\n",
    "we will try to infer the correct label $\\y^\\ip$ from the features in $\\x^\\ip$\n",
    "\n",
    "- $\\X$ is an $(m \\times n)$ matrix and $\\y$ is an $(m \\times 1)$ vector of targets.\n",
    "\n",
    "\n",
    "$\n",
    "  \\X = \\begin{pmatrix}\n",
    "  (\\x^{(1)})^T \\\\\n",
    "  (\\x^{(2)})^T\\\\\n",
    "  \\vdots \\\\\n",
    "  (\\x^{(m)})^T \\\\\n",
    "  \\end{pmatrix} = \\begin{pmatrix}\n",
    " \\x^{(1)}_1 \\ldots\\x^{(1)}_n \\\\ \n",
    "  \\x^{(2)}_1 \\ldots\\x^{(2)}_n \\\\ \n",
    "   \\vdots \\\\\n",
    "  \\x^{(m)}_1 \\ldots\\x^{(m)}_n \\\\\n",
    "  \\end{pmatrix}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <center>Training set</center>\n",
    "    </tr>\n",
    "<img src=images/mnist_small_train.png>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "h1441BMaCos9",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We will sometimes add a \"constant\" feature by setting\n",
    "$\\x^\\ip_0 = 1,  0 \\le i \\le m$\n",
    "so that the first column of $\\x$ is $1$:\n",
    "\n",
    "$\n",
    "\\X =\n",
    "\\begin{pmatrix}\n",
    "  1  &\\x^{(1)}_1  & \\ldots &\\x^{(1)}_n \\\\ \n",
    "   1 &\\x^{(2)}_1  &\\ldots  &\\x^{(2)}_n \\\\ \n",
    "   \\vdots & \\vdots & \\ldots &  \\vdots \\\\\n",
    "   1 &\\x^{(m)}_1  &\\ldots  &\\x^{(m)}_n \\\\\n",
    "  \\end{pmatrix}\n",
    "$\n",
    "\n",
    "- So each of the $m$ rows is an example and each of the $n$ columns is a feature."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "af60_0oNMM3n",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Prediction\n",
    "\n",
    "- Given training example $\\x^\\ip$, we construct a function $h$ to predict its label\n",
    "\n",
    "$\\hat{\\y}^\\ip = h(\\x^\\ip)$\n",
    "- The function $h$ will often be parameterized (by $\\Theta$) so, for clarity, we should write\n",
    "\n",
    "$\\hat{\\y}^\\ip = h(\\x^\\ip; \\Theta)$\n",
    "- We will often drop $\\Theta$ for ease of reading.\n",
    "- Since $h$ is a function, it should also be possible to make a prediction for a vector $\\mathbf{x}$ that is **not** part of the training set.\n",
    "- That is, we are able *generalize* to non-training examples:  to make out of sample predictions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Sumary**\n",
    "- a training example is a pair $(\\x^\\ip,\\y^\\ip)$ drawn from training set $\\langle \\X, \\y \\rangle$ consisting of \n",
    "    - a feature vector $\\x^\\ip$ of length $n$\n",
    "    - the associated label (target) $\\y^\\ip$\n",
    "    - $\\X$ is of dimension $m \\times n$\n",
    "    - $\\y$ is dimension $m \\times 1$, i.e., target is a single, continous value per example\n",
    "- predictions are indicated with a \"hat:\n",
    "    - $\\hat{\\y}^\\ip$ is the prediction made given $\\x^\\ip$ as input\n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fundamental assumption of Machine Learning\n",
    "\n",
    "Our goal is to learn (from training examples) to make a good prediction on a never before seen *test* example.\n",
    "\n",
    "A necessary condition is that the training examples are representative of the future test examples we will encounter.\n",
    "\n",
    "Let's imagine that there is some true (but unknown) distribution $\\pdata$ of feature/label pairs $(\\x, \\y)$.\n",
    "\n",
    "In order to learn, we must assume\n",
    "- That each test example $(\\x, \\y)$ is drawn from $\\pdata$\n",
    "- The *training examples* are a sample drawn from $\\pdata$.\n",
    "\n",
    "We sometimes call the training data an *empirical* distribution -- it is just a sample, not the \"true\" distribution.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is: our model can only generalize based on training examples\n",
    "- The training examples need to be representative of unseen examples in the wild in order to generalize well\n",
    "- Larger training sets are preferred as they may be more representative of the true $\\pdata$\n",
    "    - They should also be diverse\n",
    "    \n",
    "If the test example $\\x$ is $not$ from $\\pdata$, the model is unconstrained in its prediction."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Making it concrete: Let's predict !\n",
    "\n",
    "Let's load a dataset to make these concepts concrete"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [],
   "source": [
    "import class_helper\n",
    "%aimport class_helper\n",
    "\n",
    "clh= class_helper.Classification_Helper()\n",
    "X_digits,  y_digits = clh.load_digits()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- Let's see what $m$ (number of examples), $n$ (number of features) are"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "m=1797 training examples\n",
      "n=64 features per example\n",
      "10 classes: 0, 1, 2, 3, 4, 5, 6, 7, 8, 9\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "print(\"m={m:d} training examples\".format(m=X_digits.shape[0]))\n",
    "print(\"n={m:d} features per example\".format(m=X_digits.shape[1]))\n",
    "targets = np.unique(y_digits)\n",
    "targets.sort()\n",
    "\n",
    "print(\"{nc:d} classes: {c:s}\".format(nc=len(targets), c=\", \".join( [ str(t) for t in targets ]) ) )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Example 0, range(0.00, 1.00):\n",
      "\t  [0.     0.     0.3125 0.8125 0.5625 0.0625 0.     0.     0.     0.\n",
      " 0.8125 0.9375 0.625  0.9375 0.3125 0.     0.     0.1875 0.9375 0.125\n",
      " 0.     0.6875 0.5    0.     0.     0.25   0.75   0.     0.     0.5\n",
      " 0.5    0.     0.     0.3125 0.5    0.     0.     0.5625 0.5    0.\n",
      " 0.     0.25   0.6875 0.     0.0625 0.75   0.4375 0.     0.     0.125\n",
      " 0.875  0.3125 0.625  0.75   0.     0.     0.     0.     0.375  0.8125\n",
      " 0.625  0.     0.     0.    ]\n"
     ]
    }
   ],
   "source": [
    "# Across the features of all examples: what is the min and the max ?\n",
    "# Let's look at the feature vector for example at index ex_num\n",
    "ex_num = 0\n",
    "print(\"\\nExample {n:d}, range({mn:2.2f}, {mx:2.2f}):\\n\\t \".format(n=ex_num, \n",
    "                                                              mn=X_digits.min(), mx=X_digits.max()\n",
    "                                                             ),\n",
    "      X_digits[ex_num,:]\n",
    "     )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The dataset contains a number of examples.\n",
    "\n",
    "    - Each example $\\x^\\ip$ is a vector of 64 features, which are numbers in the range $[0,1]$\n",
    "    - The target $y^\\ip$ is a digit in the range $[0,9]$\n",
    "\n",
    "- In other words: the examples are encodings of images with labels that indicate what the image is.\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since the examples are grey scale values, we can re-arrange them into a square grid and plot:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAApgAAAHBCAYAAADNWMtrAAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4xLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy8QZhcZAAAgAElEQVR4nO3dsVYcx9b28eJbzhFXgPC5AIR9csRa58TgwE4RiR0iIjtDZHaECI8TRCoHB2K/ayHyYyMuwEZcAeIKeBOf7/v6qafVm5mCqWr+v6xbPcP0nqrqWqO9q+Zub28TAAAAUMr/mfUHAAAAwLgwwQQAAEBRTDABAABQFBNMAAAAFMUEEwAAAEUxwQQAAEBRnw38++AaRr/88kt27vvvv+8c//Of/8yu+fHHH7NzCwsLQ38upZTmIhfdk8F4fPz4MTv36tWrzvGbN2+ya54/f56dOz4+jnymquPx/v377NyLFy86x0+fPs2ucfF4+fJl5DNVHQ+995RSevfuXef42bNn2TXahvquM2YZj5QCMXH9Qe/XtQcXE9eWjKrbyIcPH7Jz+l27+3RxbKCNTDSm6v27eGi/SimlJ0+eRD5T1fFw7UP7h4vZ0dFRdm59fT3ymaqOh7tXjYfrB66/BFUdD/eM0TYTHT+CeuPBL5gAAAAoigkmAAAAimKCCQAAgKKYYAIAAKCouYG9yAcTSj///PPs3OXlZef466+/zq5xxUFv374dfF2qPMF2Y2MjO3dyctI53t3dza6JFDq45N00gnhEabvqKeioKh5aaLC2tpa9aHl5uXPsiplcQYsWUPUUMFRf5BMpvHDftSt2iBRMpcraiHL9XAv+XNGTK2pxBRBG1fF4/fp1dm5nZ6dzvLi4mF3j4uj6kVF1PNx9uQIeNT8/n53TPlThGJLFQ9uDtoWo/f397FyLhaT6HFhZWZnojU9PT7NzbpwxKPIBAADAw2CCCQAAgKKYYAIAAKCooYXWM7/99lvnWPPiUkrpjz/+6By7PE23+Lq+d08OZlU0h8XlF25ubnaOXR6Qy5Vyi5TXTvPAXDy2t7c7x1MsIj4Kml/n8g3dovuatxvMH5o57TM3NzfZNdpnoouI63Uuf6822s9dPt3h4WHn2OXh9eRoN0fHA5dbqmOIa/su/zuyyUNtIu1D+4vjXqfvHcy5m6lIXrHmtUc3r2hlDP3/af9wubZ6jXueuHufdg7CL5gAAAAoigkmAAAAimKCCQAAgKKYYAIAAKCoOxf5XF9fd46/+OKL7BpX1KO+/PLLu/7pKkUWiY4k30feZywihRdXV1cP8EnunytQUJFCA9c+lpaWJvhEsxdp61999dVE7+MWsq9dpGjBLSqvIgv0Bxcanyn9XiMFGq4tuHhof2yhMCoyFroiOOXGoki7apErYolco0UtLRSb6vjhPrOeixRIuve+6zyFXzABAABQFBNMAAAAFMUEEwAAAEVNnYPpFkyf5H1SSmlhYWGi95qlFhdDv0+RhXojeR2rq6vZOc0RaSGfrBSXP9VC/pgTyeNZXFwcvMblLrpxpXb63bp7d4uGq0guZws0t9jlCUbaUDTPbAz0vtzY4HK9W2wzKysrnWO3sLhy9+7ah/bFFnIwS3E5y1ovcddnLr9gAgAAoCgmmAAAACiKCSYAAACKYoIJAACAou5c5KOFOL/99tvga1zi/X/+85/s3DfffHPXjzNzkSTgm5ubzrFLrHbFQmMoYnEJ2HpfbuF1F6PIguRj4O7dLbbcagK63p9rI3q/0WK6SDFMbfQzu/4w6WL8LW7gsL6+3jne3NzMrtGCBBefyELaLdCCR9dftM1EC3pabB+uAFRpsY4rPj07O8vORTYBqY0WPUUWmXciRU93xS+YAAAAKIoJJgAAAIpiggkAAICimGACAACgqLnb29tP/Xv2j3/++Wfn+Msvv8xe9PPPP3eOf/nll+yaP/74IzsXKRhKKc1FLronnwxWSrGdbKLFKsFdJ6qOh9sdQJOQXWFGJEG/J0G9qnhokvTa2lr2Ii2Cc7twuGTr4C4cs4xHSoE2MjeXf0Qt9HDFGa6vjaHPRHYdcX3GFQUeHBx0jjWuf6k6Hm4M0e/Zxezi4iI7d3h42Dnu2Q2r6ni47/7k5GTwjV1xUANjyETx0PEy2j4G5kP/VXU83L1qO3djRWT3p57C49548AsmAAAAimKCCQAAgKKYYAIAAKCoOy+0/vnnn3eOf/rpp+ya77//vnP897//PbsmmG/ZHJc7qDlELp8smDvWHJc/pffv8gtdPFpcFFjzBF0OnG5e4BYSnnbB25rt7+9n53Z2djrHLm4tLooc4dq+5j65fuXi0ZNz2RR3X9ofXC7h7u5udq4n57IpkXb/4cOHiV7XItdfNHcw2j7GIFL34BZVd8/XafsLv2ACAACgKCaYAAAAKIoJJgAAAIpiggkAAICihhZaBwAAAO6EXzABAABQFBNMAAAAFMUEEwAAAEUxwQQAAEBRTDABAABQFBNMAAAAFDW0F/lEaxjp/stuH9C9vb3sXHDf3LlJPlMhRdZ00n1SU/L7gOp+uz17cTcXD90nd2lpKfS6y8vLzrGLY2owHkr7T0r53twpNdFfUpowJjpmuD1x3fcf3G95lG3EjbNjHUOUawtuT2Z3zmguHvrdR/eaHmv70HgsLCxk1+j+3Ck1MaYWicfGxkZ2jRtTtL/ctX3wCyYAAACKYoIJAACAophgAgAAoCgmmAAAAChqqMhnIpoIenZ2ll1zenqanQsm2Dbn5OSkc3x1dZVd485pYm5Pgm1ztMjnsdN4uP7ijLW/pJTSmzdvOsfah1JKaXl5+YE+zey9evWqc+zayPz8fHZurGOIFrG48fMxjzPRZ4zGyBUHtSgyhh4eHmbnxjqmvn//vnPs4uPOBYvievELJgAAAIpiggkAAICimGACAACgqKlzMPX/9lPKF291xpLrEbG9vT14zerqanauZyHxqmnOl2sfmk/mjCUeyi2G7RYRV2PJnXPceBFZMN0tDDwGrj0cHR0Nvs6NqWPoMy4eFxcXg68bw72nlOcfu9zByDPXGetzeH9/f9YfYWbcMyaSS+nyT6d97vALJgAAAIpiggkAAICimGACAACgKCaYAAAAKOrORT6afO8KNm5ubgbfZywJ+ppQ65Jp3QK3Y6ULZO/s7Mzmg1RCFzJ2BQuRRYHHVOSjMXAFLJpw7vrQWGKiBRqRgh7HJfe3SJ8xLh67u7ud4729veyajY2Nsh9sRnQBfVfQE3nmuoX4W6Tt4/j4OLsmMqaOdQ4yaVHcfYwf/IIJAACAophgAgAAoCgmmAAAACjqzjmYmmPo/r9/YWFh8H3Gki+kOXZ6nFJKi4uLnWOXTzaWBW+1fbg8l0iOlYtjizRfyuUNag7R5uZmds1Y8oVSyvN09TilfIF+XWw6pfEspK3cItE6PqytrWXXRBZTboE+U5aWlrJrrq+vB9/H5SpGNjWojfZ99+zUWgiXkxrZ4KIF2hcicwmXkzmWZ67ev4tHJGf5PsZTfsEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABF3bnIpxRN4k+pzaRb/cwusVwLFNwCwK7QQYthWuS+00gy8VgKOLSowBUZaFK2WzTZ9ZfHbiyFYFrE4Qq6IoUMY4mHFsLpovsp+XH2MYt892MpFIz0Fy14c0U+Y3nG6H1E+oabW1DkAwAAgOoxwQQAAEBRTDABAABQFBNMAAAAFDWzIp/HZH5+fvAat8PLWEWSiV1StiayjyVJ+zEVcES53a7UY4sJ/p+x9P1SIvEYS2FtxFh2CixFC39cIel9FIHxCyYAAACKYoIJAACAophgAgAAoKipczBd7qAujKsLjafkFwN1i1CPgea5LC8vZ9dcXFxk5zSPZCx5mprrsbq6ml3j8oUecw7mWHOl+mgfWVxczK4Zy8LREdr3XZ95TIuPa9938Tg+Ps7OjfUZo/fl7l0XH08p70NjGVP1Xt3zxD1zx3L/SscPNwe5D/yCCQAAgKKYYAIAAKAoJpgAAAAoigkmAAAAipq7vb2d9WcAAADAiPALJgAAAIpiggkAAICimGACAACgKCaYAAAAKIoJJgAAAIpiggkAAICihvYin2gNI90Td2NjI7vG7a0c3Et3bpLPVMhgPNy+6/v7+51jt0/sFPuMVx2PV69eZef29vY6xy4eup/9HVQdD0fbzPb2dnaN2yP3zZs3g9ek2cYjpQljonsHu33H3bjy+vXrznFPv2qujeh+9dG96bVv9byu+Xi4cebDhw/ZOTfWGM3FQ7l9x93z1e3RbTQfDzdWuHho+3DjTqosHtoX3GfWvuC+9yn2Ye+NB79gAgAAoCgmmAAAACiKCSYAAACKYoIJAACAoob2Ih9MsHXJoisrK53j+fn57BqXfO+Sso2qEmyVuy895xKw3bmgquIRSTjWZGJXGDXQLj+lqngoLUJJKS9QcG1BC3rcuQoT0lOaMElfk/Jdvzo6OsrOnZ6edo4rjMlE8Xjx4kXnWPtZSn781Di6YpjUYDy0WOni4iL2x2LjSnPx0O9+aWkp9Lrr6+vO8ViK4nReonOSPqurq53jnsLjquKhz4HIXELHk5T8symIIh8AAAA8DCaYAAAAKIoJJgAAAIoaWmh9kFu4dnl5uXPsFjnVxbbHwi1WGll4fooczKpoDo9b2FlzRtw1Lrc3urh0zVyOk96ry69z/WwM8eijMXA5qC4mwTzu5uh37caLMbcHpf1IN7NIaaqcsuZoDqrbrMH1oSk2+Kiajg2Li4vZNVdXVw/1ce6VPj/c2KDjYnBTm6nxCyYAAACKYoIJAACAophgAgAAoCgmmAAAAChq6iIfl1CqhS7umvX19Wn/dBU0edYl2msi9VgLERz33WsBh1sk2RVLjYFb4Fbj4YrAXMHCWBP0U8rv1xV9OWNtN9qPXHtw48pYigeVFim4732s9+5EnqeuKG6sdGF11zfcRg1ufK7d2tpa59gVNGlfeKhnB79gAgAAoCgmmAAAACiKCSYAAACKmru9/eRe8tk/ar6YywXSXA+X/+DOBfMCqtpoXrn70vygubn8Fq6vr7NzY4iHo/kgLr9uioVgq46HW0Rd835cPtkUi0bPMh4pTdhGdFFol0/nYhLMoaqqjWhbd21fz7k+43Lsnj9/HvlMVcVDnZycZOfcwupqrGOIo+3Bfe+vXr3KzgXzVKuKR2QTBv3uXRvSDWFSCud6VxUP5e5Bc1IPDw+za6bIP+2NB79gAgAAoCgmmAAAACiKCSYAAACKYoIJAACAou680LomCh8cHAy+xiWfj3WRaFegoUnI8/Pz2TVjjYdLONZ4uAVvx0L7y97eXnaNJpuPfUFkTcB3C8vf3Nx0jre3t7NrWlwU2dHCQNdGlEvSDxb0VE+Lt3Z2dgZf4+IxFpPEwy28PpaF5yeZg7jFx6coAquKFjBtbm5m1+icw42594FfMAEAAFAUE0wAAAAUxQQTAAAARTHBBAAAQFF33slHizZcov3FxcXgH3ZJyFtbW4PXpMpX0XeJ1JqE7Ip8XBy18Mdd8/Tp06rj4YqedCeGaPK5JiY/e/bMXVZVPDQB2xWrXF1ddY5du3e71rjYGtXv5OO+Rx1Don1Gz7XQRpTbDSxS0OR2atE20tNmqoqHFl+4oje9xj1zIoUuPYVRVcVDuQINfS7rmJKS3/1I77+F/qL36tqHFpK6eLixOLhjWlXx0HvVeVRK+fjZ8z1ndA7SU4DKTj4AAAB4GEwwAQAAUBQTTAAAABR15xzMCM2RcLlBmpuWUp4T0ZMPUVX+g3ILi+v9u/yHyGL0Lo7Pnz+vOh7uO5x0IXHN3XQL5T558qTqeDiaQ6PHKeX37q6rMH8qpQljEsnDc+c0x64nv7e5NqLfv8sddHmIGqMW89oj3PfsxocG+sy9xcONK5rb2+Iz19H+4nKY3RxkdXW1c9yzGHtV8Yjcq+Z1R+pkUpo+HvyCCQAAgKKYYAIAAKAoJpgAAAAoigkmAAAAihoq8gEAAADuhF8wAQAAUBQTTAAAABTFBBMAAABFMcEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABFfTbw74OLZP7000/ZuR9++KFzvLS0lF3z22+/ZecWFhaG/lxKlW00r5vIv379OnvRmzdvOsdPnjzJrtnY2MjO6ab1z549c5+pqnhcX193jl37+PXXXzvHv//+e3aNi9Evv/zSOf7HP/7hPlNV8VAvX77Mzh0cHHSOl5eXQ6/T9tFjlvFIycTk48ePnWN3b8fHx53j58+fZ9dov0rJtxuj6jbivleNh7tPFw8XN6PqeDx9+jQ7d3V11TleXFzMrnn16lV2roE+k8Xj/fv3neOVlZXBN3HxiIwhPf2nqnjoM9c9OyPXuLYwhv7i5gkXFxed4/n5+ewaHWNSmj4e/IIJAACAophgAgAAoCgmmAAAAChqaC/y7B81v/Lt27fZi/Sa7777LrtG8/BS6s2pU1XlP7x7965z7PJcNP9Dc9BSyvPwUkppd3e3c+xyilJl8dDcWm0LKaX05ZdfDr6xax9Df+svVcVDuTw5zT12+UL2j3267/5XdTmYmlPmcn+GXpOS70faH3tU3UbcPURyvTXPKqWULi8vO8cunzFVHg93r9oejo6OQn/s/Py8c9xCXru2857nQIcbZ25ubrJzp6enneOenLuq4qFOTk6yc5pj6J7L2qdSyttVi/3F3avm1rox140f0/YXfsEEAABAUUwwAQAAUBQTTAAAABR15xzMP//8s3Ps1q6M5Njp+9xB1fkPjubDuByaSD5ZC/lCk3BtweXjai7v999/796uuXhoe3A5Zy7HbH19PfL21eVgRmhe1fb2dnaN6zOaV9XCun4RkbUQXYxcWzKai4e2j2jesq7TO5b2od/zzs5Ods3q6mp2LrK+amowHsq1D5e7OYac1IjIWpkppbS/v985dvmdiRxMAAAAPBQmmAAAACiKCSYAAACKYoIJAACAoj676ws+//zzzrEr0NDFfV3BhiZbp+QLhsZg0oWkexZ5bZ62mb/97W/ZNV988UV27ttvv723zzRLWrDhEu0XFxcf6uNU4fDwsHPsCnpcn+kpUmheZCzoKQIcpeXl5cFrdKOKlMbbPtyi4cotvj7WeChX0OOKnnqKepqn46cr6HHx6CnqCeMXTAAAABTFBBMAAABFMcEEAABAUXdeaD1C8ytdDqbzP//zP53jnpzM5hY51fwYlyvlcj8iuZupwXgozetNKaWffvopO/f1119H3q75eLi8F5c/pTmIPXl6TS60rpsMuIWSp8hbbr6NvHjxIjvn8vA0jj2aj4cbPyObV4xlYXHtCy4eboOPYI5dc/FQ7pnrxorH8sx17d6Nse65Y7DQOgAAAB4GE0wAAAAUxQQTAAAARTHBBAAAQFH3UuSj3KLq3333XXZOiz1+/PFH93bNJ9i6ZHyXhKwJxz2LwDYfD9cWfv311+ycW9TfaD4ejkvA1iKGnoKOJot8lGv7Lkl/2qT0B1AkHq6AxRVFnp6edo7HOoa4tr+2tpad29/f7xz3FLk0H49ooaBrR0bz8XAFTq6gxxUOGqOMx+vXr7Nz07YPfsEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABFfTbtG/zwww/ZOd25xxX5uCKOb775ZtqP8+A0Cfbs7Cy7Ru/fJdPe3Nxk51wxUGvcjjwaj7dv32bXBJOLm+OKEfScSzR3r3OFYS1y96vfvyvoOTo6ys5p3+rZqaUqeq8uAV8LVtyY6kR2eKndyclJdk6Ll4I7sDRJ20dk9x3XNxxtHy2MKRoP993rs9MVOAV3/WqOG0/1XHQOotcFd376v/gFEwAAAEUxwQQAAEBRTDABAABQ1NQLrbscu3/961+Df/if//znRK9LlS1yqrkeL168GHwTl1/o8kGCeUVVxUO5xZ/1/jVnNyXfFnQh/h5Vx8MtmK75Ma4tuNw5l6tnVL/Qusv7uri46BzPz89n17hYjmGhdff96zmXZ+XaiOZQ9eSdVR2PSPtwNjc3s3PBHN2q4qHPGNfuNR6rq6vZNTs7O9m59fX1yGeqOh7umavXuDbkxopgznZV8VDuHlx+pXJtQWN01/7CL5gAAAAoigkmAAAAimKCCQAAgKKYYAIAAKCooSIfAAAA4E74BRMAAABFMcEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABFMcEEAABAUUwwAQAAUNRnA/8+0SKZHz9+7By7zeiPj48neeuUKt9o3nn16lXn+PXr19k1Hz58yM71bCyvmouHev78eXbuzZs32bmnT59G3q65eLx8+bJz7O690faR0oQx0RhsbW1l15yenmbnXFsymm8jjhtXgpqPx8HBQXbNY2of79+/7xxvbGwMXpNSE2PIRPHQ8TIaj6Dm4qFzMDf/0nnbHfTGg18wAQAAUBQTTAAAABTFBBMAAABFMcEEAABAUUNFPhPRBP1nz57dx5+pkkuU1XgEi1Wy9womZFdP4zFFAUtz3r17l53TAoXV1dXsmrHGIyXfZ7RgZX5+PrtmrOOKKz7QNrK9vf1QH+fBaXvQIsmU8ngsLy9n10TH2THQIo7HdO9O5BkzVq6gSZ877jl0H/gFEwAAAEUxwQQAAEBRTDABAABQ1NQ5mJGcQ7dIcCQnosU8EnevGiO3yKm7V10UeIrF6WfG5Xrootn7+/vZNW7RaJeL1RqXH7O4uNg5dgutu9dpPFrJSYxsxHBxcdE5dm1/LHmpkUWyNzc3O8fRzRpaHEO1HV9dXWXXaJ9xMWvx3iPcdx/pL659tDJmfMrJyUl2bm9vr3McjUeLbUbbg3vm6v0/1PfOL5gAAAAoigkmAAAAimKCCQAAgKKYYAIAAKCoqYt8XEGCJs+6JH5XDKNJ+y0UdWiC7dHRUXaNFrG4ROKbm5vs3BgSsN33rItEu2vm5uaycxo3165qownX7nvW+3DJ5y6RXfuL64s10j7j7k2LWtbX17Nr3PevhXEttBGNhytq+eqrrz75mpT8eKltqbbCKFckqufcIvu60Lor8oks4N8CLQJz37PrH2plZSU7d3l52TluschFx4qU8s0q3OYVCwsL2bnT09POsY4nNdICHveZdS7hNnNwhVDTLuDPL5gAAAAoigkmAAAAimKCCQAAgKLunIOp+VI7OzvZNS4nQmkOTUopHR4e3vXjzFxkwXjNjYvmAbmcmdrpveoCwCnl+SAuf8ppIZ9ORRbH1zbkclKdFnKUHbcQsNJcHxcTl+8cyf+ujX6PLndQx1SXy7u8vFz0cz0ElxOq36sbHyJjRm35ppPSnFz33avos1T7Ygv9JZLXrs+h6Fip791CDqbem7vXSO6ki6OORXfNYeYXTAAAABTFBBMAAABFMcEEAABAUUwwAQAAUNTc7e3tp/49+0dNgnXJ1pos6pLPXfHH9fV157gnSTtfgfvhZPHQJFhXjKCFHi6ZdnFxMTsXKSBKlcVDuUW0//3vf3eO3aKvrn0MtNX/qjoern24gjfVaPtIycREP7fbUCBSyODopgY9BVNVtxFHk+tdceX5+Xl2LrhZQ9XxcO1cnzuR50lK4cKfquPhNlTQc2dnZ9k17jmsr+tpL1XHwxUm6TzFbV7gBBdarzoejhb+7O3tZdfs7u5m54ILrffGg18wAQAAUBQTTAAAABTFBBMAAABFMcEEAABAUXcu8onQog2XKOuKg1zystFcgq0WAi0sLGTXuN2PxhoP5e5za2sr/2MjKPJxIgnYbmeO4K4b1RX5RGiS/traWnbN+vp6di6yc1JqsI1oH3HFS24HoKDm4qHPD1cI5IoHg5qLh373rhjD7fAS3DWsuXgoV7zk5iDBHX+ai4c+K1x/ieyw1oMiHwAAADwMJpgAAAAoigkmAAAAivrsPt5UF7N1iyYH88dGQfNc5ufns2seUzyUy8F0i76OVSR3rmfB39GK5M8F86VGQXPq3JjqNjVweaoYHx1DJt2o4DHpWTR8lCKbW9wHfsEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABFDS20DgAAANwJv2ACAACgKCaYAAAAKIoJJgAAAIpiggkAAICimGACAACgKCaYAAAAKGpoL/LBNYzcHsm6x6fba3oKcyXf7I6yeOj+v/v7+9mLdJ/Yi4uL0B+7vLzsHPfsnVpVPJRrH5F9tXU/+5TyPd17VBUP3QN2Y2Mje1G0PSjdZ/r4+NhdNst4pGRiouPB69evsxft7e11js/Pz0N/TNuIa0epsjai+667/hFp+258ePHiReQzVRUPHS/dfWlbj4wpd1B1PNy9TrrX9M7OTue4Z+/6quKhIvfqxl3XN4L7k1cdD9c+zs7OBt94d3c3O/fq1avIZ+qNB79gAgAAoCgmmAAAACiKCSYAAACKYoIJAACAooaKfAZpcnFKeULp0dFRds3i4mLovWp3eHjYOXbJtPPz851jl0wbKZZqgRYsuHjoOZdYXjhpf2a0gMcV9GxubnaOv/rqq+wabUMpxRP5a6P93MXEJeVHaJ8JFrnMlBZx3NzcZNdo0ZOzvLycndN+1MKYEonH2tra4Pu4Z8y7d+86xy3EQ8dU11/cM0W5e+0p6qmaFtZGiiTdNdoW+s7VLvLM1bHBja8rKytlP1jiF0wAAAAUxgQTAAAARTHBBAAAQFFT52C6hYyvrq46xy5/zOXYae5NzyLJVdE8OM2HcNe4RZNbuNcIvQ+XB6UxGsu9O9fX14PXaPtwMWs139KJ5L1tb293jt39u/dpMXc30v41Hm4MaSGfMCKSB6e5g9H+oc+YFuh46XJtgwtij8Lq6mrnOJKf3mJuZVRk/NDNLR7qecIvmAAAACiKCSYAAACKYoIJAACAophgAgAAoKipi3xcYrkuauoWynVJpmMo9tACJ3fO3XuLi8w7kUVvIwutj4Ur+lI7OzuD1+iC/im1sYj4pA4ODjrHLpHfbeDQosi4p/HQpP2UfCFDi8VhCwsLd36NWyR6LOOKFia5Z4V+z+4aVwjkisVqp/3FtXGN2evXr7Nr3PjZ4hwkUtyn9+qKIV2MpsUvmAAAACiKCSYAAACKYoIJAACAouZub28/9e+f/Mc+mgvk8tBc3tn+/n7nuCc/ZG6Sz68fXsQAABnPSURBVFRIFg/N9Tg+Ph58k62trfyNP/09fEpV8dDcH5cfozm5uoh0SlPlg1QVD237rn1oPoy7d5dfF1w0epbxSCkwhrh70zayubmZXePaVnBB5araiHLfv+aGuRxMF49gP6o6HhsbG9m5SG6zuyaYY1dVPCLPGM1bPT8/z65xfWMM/cXllu7t7Q2+sS7YntI44uFySyP56jr/Simco9sbD37BBAAAQFFMMAEAAFAUE0wAAAAUxQQTAAAARd1LkY9yibNra2vZOS326ElQrzrB1jk5Oekcu6R1l5QdXCS5uXhoIdDS0lJ2zeXlZXYusqBsajAemsQfLWoYS5GPo/frFtKeojis+Tbixga3eLIrBjKai0dkTL2PooUHUOSZ69qCK3CKFKWmBuOhcw53n7p5QUr5c7jnGdxcPHT8cP3APWMixXSJIh8AAAA8FCaYAAAAKIoJJgAAAIpiggkAAICiPpv2DTTZOqWU5ufnO8dupX3HJWrXTpNnz87Osmt0F5Ll5eXsmmBBT3Mm3T1CC4FSChf5VM3duyZcX1xcZNccHh7e10d6cNpnXGGOJuXrmJKS37FiDNyYqgVNrsArOs62xhUqud3Q1FjHVDc26rPTtY9gwUZz3L3qTmCuyMeNKWN4xrj2oc8YN8a43dKmxS+YAAAAKIoJJgAAAIpiggkAAICipl5o3S3Y6RYwVe7/+1tcFFhz6lweqeaDTLGIuFNVPFRk0XB3TXDBbKeqeEQWlddcINenpsivq26hde0zbtMFjYnLXZ0ix66qNqI5ZG4s0EWyXT6dW0g7qKp4qMjY6J4dbrHxoKriERkv9V7d+DnW9uFysY+Ojgbf2OW1B/O6q46Hax+ac7m6uppdE6mN6MFC6wAAAHgYTDABAABQFBNMAAAAFMUEEwAAAEUNFfkAAAAAd8IvmAAAACiKCSYAAACKYoIJAACAophgAgAAoCgmmAAAACiKCSYAAACK+mzg3ydaw0j3RdW9dlPye+kGVb0PqKP7SLt9YnXP6pTCe8dWHQ/33Ufax/HxcXYuuPd01fFwdJ/Y7e3t7Bq3T2xw//rq9iKP0D2B3XjhYjKGPuPo/vSuz7j9uIOai4e2j+gYElR1PHS8SCkfM9zzZApVx8PR9uHaghs/xvqMmVV/4RdMAAAAFMUEEwAAAEUxwQQAAEBRTDABAABQ1FCRzyCXcHx2dtY53t3dnfbPNE2TiV0hQrA4oTmuoOni4qJzvLy8nF0z1ni45GrtHy7RPFjQ0wSNgSagR68Zaxtxfebg4KBzvL+//1AfZ+a0wCmllI6OjjrHh4eHD/VxZs599xsbGzP4JHVw/UXbx/r6enZNsKCnOa4gUuOxubn5IJ+FXzABAABQFBNMAAAAFMUEEwAAAEXN3d5+ct3OwUU9XR6D5tidn5+HXhdU9SKnLv9hZWWlc+xyaFyeUVBV8dAFfpeWlgbf5DG1D5ezrPlTl5eX2TVT5GBWt9B6pI2srq52jt2iyFOouo1E2v4Ui8w7VcfD3Zf2hyk27nCai4e2h8L5hVXHw92rjjFTbGTiVB0P3dglpZT29va6b/Lped9dsdA6AAAAHgYTTAAAABTFBBMAAABFMcEEAABAUVMvtO4WjtaFs8e6oKkTSTYf86K4rj0oLeAY0yLiQ66vr7Nz8/PznWMtkktpXDFyCfdqrIuoR7j4aOL+Y4qPu9fH9EzRMfXm5ia75jHFQ7n+8vz5887xY+ovrgBwcXHx4T9I4hdMAAAAFMYEEwAAAEUxwQQAAEBR95KDqflibjN6l4c4hjyzSA6iW1ha81ZTyhdHXV9fn/yDPZCrq6vBazRn5uzsLLumhXuN0PbgFtTXnKrNzc3sGs0pSimlN2/edI7HlGekeURu8WAXE3eudnqvLsdO7//4+Di7xo2pU2zgMDPaZ9yYqmOIax8vXrzIzo3hGeNoPFz7cHmaLfYX5fpLpH24vjGGMdTlpOpz2d2nm6e5PnQX/IIJAACAophgAgAAoCgmmAAAACiKCSYAAACKmru9vf3Uv3/yH1PyicO6ULQrYHGLSZ+fnw++d0ppbugz3aPBeLjkWU1C3t7eDv0xTdTuWaC6qnjoQvMrKyvF/tjh4WHnuCcBuap46HfmCjG0iMEt1u9ep/3DJWmn2cYjJRMTLWpZW1sbfJPoGDKGNuKKAHVzAle04L5/HY+0MOwvVcVDuUIUVxgYoQV0PTGrKh73Oabqs6jCMaTIM9fRDS5Syu+/hfFDRfqLjicp+efOtPHgF0wAAAAUxQQTAAAARTHBBAAAQFFMMAEAAFDU1Dv5uKTPnZ2dzrHbPcEVrGhRS0+RT9W0gCElX6Ch3K4CBwcHnWMXs9p2ptDPs7i4mF0T2e3H0QKFaXcZeAiRXa20yMUVYkR2iGqFJqHv7+9n1+gY4r5rFyctfmmxjezu7mbX6L26IgY3FrgdXVrjxs9I0YJzdHTUOXbto7bdbfQ56IpTlPveXX/RZ0xPkU/VXPvQ7zn6HNLxw7WF2p65KtJf3D24Z8y04ym/YAIAAKAoJpgAAAAoigkmAAAAirqXHEzNFXS5Hy63IZKrWDuXN6r5lW6RZM2FSSml9fX1znHtuR8p5YveulwgjYdb4NUtnjuG9uHavcbI3afLu2ohvzDC5R8r12cczb1qkbtX7fsub9XlaI+hz7j2EXnGuLFY+1pt+ZYRbkzVscBtXuDyVF07as0UGwxktM208MxVkf7iakXcvU47fvALJgAAAIpiggkAAICimGACAACgKCaYAAAAKGru9vZ21p8BAAAAI8IvmAAAACiKCSYAAACKYoIJAACAophgAgAAoCgmmAAAACiKCSYAAACKGtqLfHANI7cP6MePHzvHbu/Ui4uL7Jzut+z21n3y5Mnc0Ge6R4Px0HtPKY+Ri5nb8zO4f2rV8XB0r+XoPrG6f2rPPrHNxUP3jo3uq+z2kzVmGY+UAjFxe+ceHBxM9Md0rFlfX3eXVd1G3r9/n53TGJ2dnYX+2OHhYee4Z//6quPh+oOOoe55oveeUu/9q6rj4dqHPj+urq6yaxYXF7Nz7hlrVB0Pdw86DugzJyX//NAxVfc0/0vV8Zh0/Njc3MzOTTsH4RdMAAAAFMUEEwAAAEUxwQQAAEBRTDABAABQ1NBe5BMV+ShXoBApDuopYqgqwVYTal0SuSYh9yQOD76uR1XxiNDEYfc9u8Kw58+fD16TKo+Hu1eXlK1cf2mgfaRkYqL93I0P+l27hPy9vb3s3O7ubufYJfenytuIG0M0ZhqflFLa2dnJzmmRUwt9RvvDyspK9iItWHHFlTc3N9m56+vrznGLRRyuv0S4QqjLy8vOcYuFk+4z6zkXM9cXtO+1MH7oc8CNDT3f66BpC0n5BRMAAABFMcEEAABAUUwwAQAAUNTQQuuD3CLJyuUxuPyx4P/3V0UXtHW5HpPkaY6Z3r+Lh8sZieau1szlx7hzyi14q+1q0tysh6bfY6TtR3K9U/IbFrQmsrixi5mLUYt9Rtuxy5XT3FL3jHE5upqr2WJ83HNS78M9l12bmTQ3ryYuHpH7iuS+t0Bza93zRMeUSJ53CfyCCQAAgKKYYAIAAKAoJpgAAAAoigkmAAAAirpzkY8m1EYKc6IJ+prM7RJRa6PJ5roAcEopnZycdI5d0rorTtCk7BYSsvUzR4pTXHKxFk+l1Mb9K+0f7rufpOglpTbjkVL+/buY6DXRAkAt9uhZWLwqOj4cHh5m12jhS7TPRIowa6dj7DTuo5DhobmCJu0vroDFLTyvz9hIgVltJh0HXVtocUzV/uH6S2QTm/uYg/ALJgAAAIpiggkAAICimGACAACgqDvnYOr/wbtcj0i+lMuNiiw4XbuVlZWJXnd0dJSd0/yHFhai1xwet9jxpFpcFFkdHBxk5+bn5zvHLlfKaTFnOaX8c0faiMYoJZ9r5PLTand6eto51pzMvnMR2h+j+fCzpM8U91yI9hHVYjx03HdjSITrLy3m6GpfKLm5wsLCQrH3mpXopgPKtSvN3bxrji6/YAIAAKAoJpgAAAAoigkmAAAAimKCCQAAgKLmbm9vP/Xvn/zH3jedm+scu4KeKRbPnRu+5N5k8dAkWJc0rknabiFtV6Chycu62PJfqorHJFyStito0rj1FP00Hw+3mK3rQz3tQc0yHikFYuL6w9LSUud4f38/u2aKAoWq2oiOIa5wUr9/l5DvFmgPFn5VFQ8t4tjd3c1epDFzi8xHisBaHFPd2Kjtw40X7nXBhbOrioeOFxcXF9mLLi8vO8euOMUttK4x6olPVfFQ7l63trY6x8vLy9k1bqwIjrG98eAXTAAAABTFBBMAAABFMcEEAABAUUwwAQAAUNTURT4uCVSTTF0S/xS7slSdYOto8qxL4nfngpqLh7YZV7AwRVFH8/GYdHesHtUX+UR2nri+vs6ueUxjiBbCuTbixtmg5uKhu/tE4xFsM1XHw92XFiu5go0pdi2qOh6OFvC4Yh23e16w+Li5eMyq8JpfMAEAAFAUE0wAAAAUxQQTAAAARX027Ru43DDNbZgiV2oUNB9E84ceG82Xii76OlaaU+UWnh8zlw+0ubnZOX5MY4gbU3XxcRezsXLxODs76xy7nO2xthmXg6k5hi6v+THR54d7nkyRc1g1HStSyp+xD3Xv/IIJAACAophgAgAAoCgmmAAAACiKCSYAAACKGlpoHQAAALgTfsEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABFMcEEAABAUUwwAQAAUBQTTAAAABT12cC/Dy6S+fLly+zc8fFx59htNO9e9+TJk6E/l1JKc5GL7slgPNy9Pnv2rHP85s2b7Jrnz59n516/fh35TM3F4+PHj51j1xZcPIKqjkfkO3Xt4+LiIjt3enraOe6J2SzjkVIgJhsbG9k5HQs+fPiQXfPq1avsXLDdVN1G3r17N3hub28vu0bH3ZRSWl9fj3ymquLx/v37zvHV1VX2ou3t7c6xe3a4ccWNR0ZV8dDx0rX7g4ODzvH8/Hx2jetnbqwxqoqHcv1F+4K7xvWhFvuL0v7jzkXaUErTx4NfMAEAAFAUE0wAAAAUxQQTAAAARTHBBAAAQFFzt7efzBkdTCh1SfUuIV89ffo0O+cScY2qE2xdYrkmabv4RBJzXcxS5fFwSfWagN1zXxltHz1FYVXHQwu+HNeGXDK+Ju2716UGinzcGKJ9xl3jktKvr687xy22Efc96r3u7u5m17gxxBX+GFXFQ+/ffc+rq6udYzeGuHG2xWeM9v2tra3sRdoeXLvf2dnJzp2fn3eOe8anquKhY8PCwsLgm2h7ScnHqMX+olwhqSvqUa4PuTHFoMgHAAAAD4MJJgAAAIpiggkAAICihhZaH+RyNvT/8l3+mMt/0PyYKRbbnhm3mK3mRLhcBxePaG5izdw96Pfq8l4i+SAttg/XXzRPNdpfgotGVy+yAHQkrzul8GYNVVtbW8vORRbbdm1L49bCmBLJU9YxdYqNO6qn/dyNl9pf3OL0btHsFtqD0u/V3Ze2oZWVlewaN+6MgXsuasxc37iP/sIvmAAAACiKCSYAAACKYoIJAACAophgAgAAoKipi3xcoYEm1LoE/bEWtbgE9ZOTk87x5uZmdo0r7BiDSDFCtC20WNSj3H1pUYdrH8EFgJvkxhBdFFqLXFJK6fT09L4+0ky5ogVdSNstGB4thKqdFl+4haP13NnZWXbN4eFh2Q82I5GNOrSox937WIoClXsuaEGoGz/dODsGbg6i7WOKRebvhF8wAQAAUBQTTAAAABTFBBMAAABFTZ2D6XKjlMuPuby8zM6NIQfTLd66vLzcOXa5MGNZFFjbg7vXi4uLwffRHJpW6X1Ecm3Hmo/bxy2SrVz/GENObpTeq1uMXfM0U2pzTNXv2uWUHR0ddY5dPt1Ycg413zSSaxtZrH4s3L1qDrc+g1Ma7zgbaR9unsJC6wAAAKgeE0wAAAAUxQQTAAAARTHBBAAAQFFzt7e3n/r37B+1aEEXVU8pTzZ3SaeuiEMX+uxJUJ9zJx/IJ4OVki880PtyCbZu4eSgquKhRT7uO9R7dcnWrngsmJRdVTzcQvNK+8LW1lZ2jVt8O2iW8Ugp0GdcG9HCH7cIsOtHkYKhVFkbUa7NaKGHS8g/ODjIzgXbTVXxiIwhNzc3nePCm1dUFQ/dqGN7ezt7kT533Pg5xULaVcVDuSIfnXO4a8byzFWRImLXFqbYqKE3HvyCCQAAgKKYYAIAAKAoJpgAAAAoigkmAAAAirrzTj6acD0/P59do4n2LnnUFQdpUnakQGLWIgnpes0UybTV02RiV4wQ2d3GFXC0SJPv3b1qmzk/P8+umaLIpzqR5HpNyndJ6S6WwSKfqu3t7WXn9P5doYcbi1ukY4YW9KSU0urqaudYd/ZJaTw7tWjfdwVN2l90J5uUfGHtGHb8cc9cHXfd+KHFUymNY5x18wudS7m+4V437U5g/IIJAACAophgAgAAoCgmmAAAACjqzjmYmh/jFhZfWFjoHLvcIJfr0GL+lMbDLXCrMXILIo+VW/RV80Fcrq17XYs0x8m1D80fOzw8vNfPNGsaE5cHpt+/ywXSxcfHwn3/mpOsOYgpjSOfznE5ye65o8aaY+fuXXMudbOTlMbbPlx+pY4fV1dX2TVjzXV3z1MdP1xtxLT5lg6/YAIAAKAoJpgAAAAoigkmAAAAimKCCQAAgKLmbm9vZ/0ZAAAAMCL8ggkAAICimGACAACgKCaYAAAAKIoJJgAAAIpiggkAAICimGACAACgqKG9yAfXMNI9LlPK91t+9+7dXT7TkLmSb3ZHg/F48+ZNdk73BnXXRPbW7VF1PNz+8toe3L7jU+xLX3U8Pnz4kJ3TPuT6lNtfNmiW8UgpEBO3R7Kec/uOu/10g6pqIzpeuu/64OCgc7y8vJxdM0U/qioeyo2Xek5jmJK/dxcjo6p46L1FnrmuvzzmZ4xrH9qnUgrvRV51PL777rvs3M8//9w5fvv2bXbN119/Peln6o0Hv2ACAACgKCaYAAAAKIoJJgAAAIpiggkAAICihvYiz/5RixSWlpYm+sMuSf39+/eRl1adYOsSsE9OTjrHm5ub2TUukT2o6ni4xPKzs7PBN768vMzOPX36NPKZqo6H+563trYG3/jw8DA710DBQkomJtofXJ9ZXFzsHLtE/rEUgmlBhivQ0Hbj2pErIAsWWFYVD7031z+2t7c7x+7Z4cYZHVd6xpSq4qHf4dra2kRvfH5+np1zBXZGc/HQZ6xrHxcXF/kf+/R86L+qiof6/PPPs3PX19ed44WFhewaVxz0/fffRz4TRT4AAAB4GEwwAQAAUBQTTAAAABQ1tNB6xi1YqlZXVzvHLs+l8OLr1XA5LZr/cXR0lF3jFlcO5hw2R/NvXS6Max/BnMOqudyX+fn5zrH73o+Pj7NzrcZD79fRe3N5iVPkYFZFF4x3Y6x+/24MCS4SXb3IM0a5HDuX5z/WMTUiktvbAs3rd9+z9pfopgza9qbYzGFmfvjhh+yc5lL+/e9/z67517/+lZ379ttvO8fu+fUp/IIJAACAophgAgAAoCgmmAAAACiKCSYAAACKuvNC65oE65I+dVFPt5CyS8oOJndXvcipuwd3/8olE7vCDqPqeEy6sLgutJ2SX0jaqDoejibf7+zsZNe4wpgG+ktKgc0a3GL8V1dXg2/c6OLzg23EFS9FYuaKOMbQZ9z4qUWANzc32TWnp6fZORc3o+p4RMZUN366PjWGhcUdnV+4olE3zupi9D0L0TcXD11E/T//+U92ze+//56de/v2bef466+/dm/PQusAAAB4GEwwAQAAUBQTTAAAABR15xxM5XIUdOHs7e3t7BqXR6J5Ez2L4jaX/xDh8ox08fWx5IMolye2tLSUnRtrfkxEZPH1CuOR0oR5yzoWuHxkl1el53oWSh5lG3H3OtYxVXMp3X1NsYh4c/GImJvLb0vzVHtyVJuPhxsr1tbW8j820pzUiG+++WbwGs3J/As5mAAAAHgYTDABAABQFBNMAAAAFMUEEwAAAEVNXeTjaGK5Kz6ILCbcs9D4KBNsT05OsnO6kPRjiodrH1oM0pPEP8p4uEW0NXG9wvaR0j3GJDKu9Cy8Pso24goF9VyL8XCbcqysrHSOp1hU3ak6HpNy/UXbgxt30wji4QoJ3SYxj7mQ9KeffsrO/fjjj51j3UTnLxT5AAAA4GEwwQQAAEBRTDABAABQFBNMAAAAFPXZfbypJsa6xGFXoNFTpNCUyK4kzuXlZXZOC3/cjjc9O3NUw917pDjF3evV1VXnWHc6Sqn+eDgaI9eGXLL5zs5O57jF9pGSL3DTZHst6kjJF3Fom+gpaqmKfv/uM2uxjouH62tbW1vTfbgKRNpwC+18UjoeuPFBz7mdayZ9NtVG78N990dHR51jHU/69Oz81ZTffvstO/fLL790jl2xzs8//1z8s/ALJgAAAIpiggkAAICimGACAACgqKlzMF1+ZSSnzOWI9Cxq2hSXW6q5cs7y8nJ2bn19vXPcYn6I+54jubYur0bbR4t5Vy5PUnMJ3X25PuTaTIt2d3cHr3ELzd/c3GTnWoyJtmuXW6rjiutD7nWrq6vTfLQquHvVsbHFsSBK+77L0T07O+scLy4uZte4GPUsrF41fQ66XPzNzc3OsRsrtre3s3NjaEd//vlndu7XX38dfN23336bndOF1u+KXzABAABQFBNMAAAAFMUEEwAAAEUxwQQAAEBRc7e3t7P+DAAAABgRfsEEAABAUUwwAQAAUBQTTAAAABTFBBMAAABFMcEEAABAUUwwAQAAUNT/Ak1nyr2PjhKbAAAAAElFTkSuQmCC\n",
      "text/plain": [
       "<Figure size 864x576 with 100 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "fig, axs = clh.plot_digits(X_digits,  y_digits)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Our problem is to take an unknown $\\x$ and map it (predict) to a label in the range $[0,9]$.\n",
    "\n",
    "- This is a *classification* problem as our predictions are from a finite set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true,
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN score: 0.990000\n",
      "LogisticRegression score: 1.000000\n"
     ]
    },
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 864x216 with 10 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "Xd_train, Xd_test, yd_train, yd_test, models = clh.fit_digits(X_digits, y_digits)\n",
    "\n",
    "_= clh.predict_digits(models[\"knn\"], Xd_test[:10], yd_test[:10])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "- How would **you** predict a label for an image, given the 64 pixel values ?\n",
    "\n",
    "- We will use a very simple (and inefficient) algorithm called *K Nearest Neighbors (KNN)*."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "WAwEIaJ9UmAC",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Template matching\n",
    "\n",
    "$\\newcommand{\\vc}{\\mathbf{v}_{(c)}}$\n",
    "\n",
    "- One approach to Classification is to match our input vector $\\x$ against a *template*:\n",
    "(a vector of similar length) whose class is known.\n",
    "\n",
    "- With one template $\\vc$ for each class $\\c \\in C$, we could classify $\\x$ as being in the class $c'$\n",
    "whose template was \"closest\" to $\\x$.\n",
    "\n",
    "- We need a similarity measure that maps $\\x$ and $\\vc$ to a number such that\n",
    "larger means more similar.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuTufoJG2R6N",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Our first predictor: K Nearest Neighbors (KNN)\n",
    "\n",
    "- Here's one of the simplest Machine Learning algorithms, that leverages template matching.\n",
    "- In this case, the templates are the feature vectors of the training set.\n",
    "\n",
    "- Use the similarity measure to find the $K$ training examples closest to $\\x$; \n",
    "\n",
    "- Predict the class that appears most frequently among these $K$ examples."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuTufoJG2R6N",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Here is our predictor function, given *test* input $\\x$:\n",
    "    - For each training example $\\x^\\ip$, compute the similarity $s^\\ip$ of $\\x$ to $\\x^\\ip$\n",
    "    - Let $S_K$ be the set of $K$ training examples $i_1, i_2, \\ldots, i_K$with greatest similarity to $\\x$\n",
    "        - $Y = [ \\y^{(j)}  | \\ j \\in S_K ]$ be the classes associated with these closest examples\n",
    "    - Let $\\text{count}_c$ be the number of elements of $Y$ that are equal to class $c, c \\in C$.\n",
    "    - Predict class $c'$, with the greatest $\\text{count}_{c'}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "vuTufoJG2R6N",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- KNN operates under the assumption (Manifold Hypothesis) that if two vectors are similar, they have the same class.\n",
    "\n",
    "- If $K=1$, the predictions are highly sensitive to the training examples; increasing $K$ may increase\n",
    "the prediction accuracy.\n",
    "  \n",
    "- Although simple, can you spot the drawback to KNN ?\n",
    "\n",
    "    - The size of $\\Theta$ (the number of parameters) is proportional to\n",
    "        - the size of the training set: $m * n$\n",
    "        - ideally: $m$ is very large\n",
    "  \n",
    "  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "KNN is so simple it's almost embarassing to call it Machine Learning.\n",
    "But it does illustrate the key steps\n",
    "- the basis of Supervised Learning are training examples\n",
    "    - the more the better\n",
    "- the training examples are used to *fit* a predictor\n",
    "    - we will learn many predictors (models) in this course\n",
    "- the features of the examples are the key to prediction"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- KNN did not make intelligent use of the features: it merely  memorized the $m$ examples.\n",
    "\n",
    "    - That is, it used $m$ templates each of size $n$ so $|\\Theta| = m*n$.\n",
    "\n",
    "- We will see that many ML algorithms, both Classic (e.g., Regression) and Deep Learning,\n",
    "are based on *solving* for $\\Theta$ -- finding small templates that are effective\n",
    "for prediction.\n",
    "\n",
    "- A more intelligent basis for prediction would include:\n",
    "    - finding one (or more) features that are predictive\n",
    "    - finding relationships among features that are predictive\n",
    "    - find a subset of features that is *common across all examples* in a class.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Another issue: perhaps we are using the wrong features ?\n",
    "    - are $n = 64$ raw pixels the best representation of the input (and template) for learning ?\n",
    "    - would higher level features (e.g., groups of pixels that form horizontal/vertical lines) be more efficient ?\n",
    "\n",
    "\n",
    "- This is called Data Transformation or Feature Engineering and will be key concept."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Fitting a predictor (training a model)\n",
    "\n",
    "- As simple as KNN is, it has far less mathematical basis than most ML algorithms.\n",
    "\n",
    "- To be more formal, a predictor is a function of feature vectors $\\x$ whose behavior\n",
    "is parameterized by $\\Theta$.\n",
    "    - terminology: also called an *estimator*, *fitted model*\n",
    "\n",
    "- A *cost or loss function*  measures how well the predictor performs\n",
    "on the training set, given  $\\Theta$.\n",
    "\n",
    "- Fitting (or training) the predictor: solve for the $\\Theta$\n",
    "that minimizes the cost function.\n",
    "\n",
    "**Note** The loss function is relative to the *training* set\n",
    "-  it is usually similar, but not\n",
    "identical, to the Performance Measure that quantifies how well the predictor peforms\n",
    "out of sample (e.g., on the Test or Validation set)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6lKaQvuHCc1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Cost/Loss, Utility\n",
    "\n",
    "- The prediction $\\hat{\\y}^{(i)}$ for example $\\x^\\ip$ is perfect if it matches the true label $\\y^\\ip$\n",
    "\n",
    "$ \\hat{\\y}^\\ip = \\y^\\ip$\n",
    "\n",
    "- Perfection  is hard (at least at first) so we need a measure for \"how far off\" the prediction is.\n",
    "\n",
    "- We will call the distance between $\\hat{\\y}^{(i)}, y^\\ip$ the *Loss* (or *Cost*) for example $i$:\n",
    "\n",
    "$$\n",
    "\\loss^\\ip_\\Theta =  L( \\;  h(\\x^\\ip; \\Theta),  \\y^\\ip \\;) = L( \\hat{\\y}^\\ip , \\y) \n",
    "$$\n",
    "\n",
    "where $L(a,b)$ is a function that is $0$ when $a = b$ and increasing as $a$ increasingly differs from $b$.\n",
    "\n",
    "Two common forms of $L$ are Mean Squared Error (for Regression) and Cross Entropy Loss (for classification)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6lKaQvuHCc1",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Loss for the entire training set is simply the average (across examples) of the Loss for the example\n",
    "\n",
    "$$\n",
    "\\loss_\\Theta  = { 1\\over{m} } \\sum_{i=1}^m \\loss^\\ip_\\Theta\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Whereas Loss describes how \"bad\" our prediction is, we sometimes refer to the converse -- how \"good\" the prediction is.\n",
    "\n",
    "We call the \"goodness\" of the prediction  the *Utility* $U_\\Theta$.\n",
    "\n",
    "So we could state the optimization objective either as\n",
    "\"minimize Cost\" or \"maximize Utility\".\n",
    "\n",
    "By convention, the DL optimization problem is usually framed as one of minimization (of cost or loss) \n",
    "rather than maximization of utility."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Since Cost is inversely related to Utility, you will sometimes see\n",
    "the minimization objective written as\n",
    "\"minimize -1 times Utility\".\n",
    "\n",
    "So be forewarned that you will often see Loss function with leading \"negation\" signs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "m6lKaQvuHCc1",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "### Creating Loss functions is a key part of Deep Learning\n",
    "\n",
    "As you will come to see, particularly for Deep Learning, the essence of many problems is in creating a Loss Function that captures the objective of your problem.\n",
    "\n",
    "This is  far from a trivial part of the process."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHoiCEeMRxG-",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Optimization: Minimize Cost/Loss, Maximize Utility\n",
    "\n",
    "- The goal of fitting/training is to solve for the $\\Theta$ that minimizes the training set loss \n",
    "$L_\\Theta$ (or conversely, maximizes the Utility $U_\\Theta$.\n",
    "\n",
    "- The method for finding $\\Theta$ is called optimization.\n",
    "\n",
    "- There is one optimization method that we will study in depth: Gradient Descent.\n",
    "\n",
    "- We can use this in Classical ML but it will become a key tool once we move on to Deep Learning.\n",
    "\n",
    "- One focus of this course will be variations on Gradient Descent."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "SHoiCEeMRxG-",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- The difficulty of finding a good \"solution\" to a problem is\n",
    "    - creating a loss function that describes your objectives, which often have multiple aspects\n",
    "    - having a large and diverse set of training examples to estimate $\\Theta$.\n",
    "\n",
    "- Many packages have a method \"fit\" that takes the training set and performs the optimization.\n",
    "\n",
    "- These packages usually create a \"model\" object (containing the $\\Theta$ among other things)\n",
    "\n",
    "- We use *predictor* and *model* as synonyms."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# The dot product: a common Utility function\n",
    "\n",
    "- The \"dot product\" (special case of inner product) is one function\n",
    "that often appears in template matching\n",
    "\n",
    "- It measures the\n",
    "similarity of two vectors\n",
    "\n",
    "$\n",
    "\\mathbf{v} \\cdot \\mathbf{v}' = \\sum_{i=1}^n \\mathbf{v}_i \\mathbf{v}'_i\n",
    "$\n",
    "\n",
    "- As a similarity measure (rather than as a distance) high dot product means \"more similar\"."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- There are several intuitions for the dot product\n",
    "\n",
    "- The dot product is maximized  when large (resp., small) values appear in similar positions in both vectors\n",
    "  - this becomes evern more obvious if we $0$-center both vectors such that \"small\" values become negative\n",
    "  - this looks like the statistical formula for covariance\n",
    "    - if we normalize both vectors to unit length, then this looks like correlation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    " - Geometric.  Let $\\alpha$ be the angle between vectors.\n",
    "    - $\\mathbf{v} \\cdot \\mathbf{v}' =  || \\mathbf{v} || *  || \\mathbf{v}' || * \\cos(\\alpha)$\n",
    "    - $\\mathbf{v} \\cdot \\mathbf{v}' \\over{ || \\mathbf{v} || *  || \\mathbf{v}' ||}$ is called the cosine similarity\n",
    "      - similarity between normalized vectors\n",
    "    - similarity is maximized when $\\alpha = 0$, that is $v$ and $v'$ are coincident (but perhaps different lengths)\n",
    "    - similarity is $0$ when $v, v'$ are orthogonal\n",
    "    - similarity is negative when $v, v'$ point in different directions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "aojGEqYcw9Z5",
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- We can generalize dot product to higher dimensions by taking the sum of element-wise multiplication (or simply by first flattening both vectors to one dimension)\n",
    "\n",
    "- We will see the dot product appear repeatedly, particularly in Deep Learning."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Summary\n",
    "\n",
    "- Machine Learning is a *process* that involves multiple steps\n",
    "    - It is *not* just learning to use various models (predictors)\n",
    "    - We will emphasize the process as much as the algorithms\n",
    "- Supervised Machine Learning depends on the availability of data\n",
    "    - obtaining, cleaning, augmenting data is important\n",
    "- An example is a collection of \"features\"\n",
    "    - finding/creating/interpretting features is the key skill of a Data Scientist\n",
    "        - which features are important \n",
    "        - how do features interact\n",
    "    - sometimes features are missing or too low level\n",
    "    - a key skill is creating features than enable learning\n",
    "    ML\n",
    "\n",
    "\n",
    "- A key part of Machine Learning is stating an optimization objective that captures your goal\n",
    "    - not always obvious"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<img src=images/ML_process.jpg>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "colab": {
   "name": "Overview_and_notation.ipynb",
   "provenance": [],
   "version": "0.3.2"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
