{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "source": [
    "$$\n",
    "\\newcommand{\\V}{\\mathbf{V}}\n",
    "\\newcommand{\\v}{\\mathbf{v}}\n",
    "\\newcommand{\\offset}{o}\n",
    "\\newcommand{\\o}{o}\n",
    "\\newcommand{\\E}{\\mathbf{E}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Natural Language Processing\n",
    "\n",
    "The datasets for Machine Learning have historically been mainly numeric.\n",
    "\n",
    "But non-numeric data such as Image and Text is an abundant and potentially rich source of insight.\n",
    "\n",
    "We have illustrated many of the concepts in this course with Image data.\n",
    "\n",
    "We will briefly dive into the world of text.\n",
    "  \n",
    "*Natural Language Processing* is the set of tools/techniques that facilitate using text as raw material."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## The world of text\n",
    "\n",
    "- SEC filing\n",
    "- analyst reports\n",
    "- news articles\n",
    "- tweets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will approach text mainly from a Deep Learning perspective\n",
    "- lots of data\n",
    "- minimal pre-processing\n",
    "- \"feature engineering\" by the Neural Network"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "That is not to discount more \"classical\" methods for NLP\n",
    "- Part of speech\n",
    "- Stemming\n",
    "- Lemmatization\n",
    "- n-grams\n",
    "\n",
    "All of these are potentially useful as pre-processing steps for Deep Learning.\n",
    "\n",
    "However, if our data sets are big enough, it may be counter-productive to preprocess."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issues with text\n",
    "\n",
    "There are several big issues to tackle regarding text data\n",
    "- Words are categorical variables\n",
    "- Token sequences (sentences/paragraphs) are variable length\n",
    "- Token sequences: order matters\n",
    "\n",
    "We are using the term \"token\" rather than word\n",
    "- tokens may include punctuation, special characters\n",
    "- tokens may be characters rather than entire words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "## Notation\n",
    "- $\\w$ is a sequence of $n_\\w$ tokens $\\w_{(1)}, \\ldots, \\w_{(n_\\w)}$\n",
    "- each token is an element of vocabulary $\\V: \\w_\\tp \\in \\V, 1 \\le \\tt \\le ||\\w||$\n",
    "    - token $j$ in vocabulary $\\V$ is denoted $\\V_j$\n",
    "- We define two pseuduo-tokens to denote the start/end of the sentence\n",
    "    - $\\w_(0) = \\text{<START>}$\n",
    "    - $\\w_{(n_\\w+1)} =\\text{<END>}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We need a function to convert a token into a numeric vector:\n",
    "$$\\text{rep}: \\text{token} \\mapsto \\mathbb{R}^{n_\\V}\n",
    "$$\n",
    "\n",
    "\n",
    "One Hot Encoding (OHE) and word embeddings are examples of such a function.\n",
    "\n",
    "- For OHE: $n_\\V = ||\\V||$\n",
    "- For Word Embeddings: $n_\\V$ is the dimension of the embedding vector\n",
    "\n",
    "We will extend $\\text{rep}$ to sequences $\\w$:\n",
    "$$\\text{rep}(\\w) = \\left[ \\text{rep}(\\w_\\tp) | 1 \\le \\tt \\le ||\\w||  \\right]$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issue 1: Words are categorical variables\n",
    "\n",
    "We address the first issue relating to text: words are *categorical variables*.\n",
    "\n",
    "By now, we should know to **not** treat categorical variables as ordinals.\n",
    "\n",
    "Let's review the reason."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Treating a word as an ordinal \n",
    "$$\\text{rep}(w) \\in \\mathbb{R}^1$$ \n",
    "would imply\n",
    "- \"apple\" < \"orange\" is a sensible statement\n",
    "- that this ordering is meaningful to a Machine Learning model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "**Example**\n",
    "\n",
    "Linear regression:\n",
    "$$\n",
    "\\y = \\Theta^T \\text{rep}(w)\n",
    "$$\n",
    "\n",
    "Predict $\\y$ given feature vector (attributes) $\\text{rep}(w)$\n",
    "- by learning parameters $\\Theta$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose that we tried to encode word $w$ with an integer:  $\\text{rep}(w) = I_w$.\n",
    "- $I_\\text{apple} = 10 * I_\\text{orange}$\n",
    "    - means \"apple\" has 10 times the impact on prediction $\\hat{\\y}$ as \"orange\"\n",
    "    - impact is $\\Theta * I_w$\n",
    "- Re-encoding \"apple\" with a value 10 times larger would make it 10 times more important\n",
    "       "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Sparse Represention of words by One Hot Encoding (OHE)\n",
    "\n",
    "So the natural way of representing a word is as a categorical variable\n",
    "- indictor per word: $\\text{Is}_\\text{apple}$\n",
    "- One Hot Encoding"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "OHE is a *sparse* representation\n",
    "- length of $\\text{rep}(w)$ is $| \\V |$, yet only a single non-zero element\n",
    "\n",
    "The problem is that there are lots of words !\n",
    "- $|V|$ is large !\n",
    "- $\\text{rep}(\\w)$ length is $|\\w| |\\V|$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Issue 1 revisited: Sparse verus dense representation of categoricals\n",
    "\n",
    "## Dense representation of words: Embeddings\n",
    "\n",
    "Sparse encodings, such as OHE\n",
    "- convert a token into a vector of features\n",
    "- where the features are orthogonal: only one is active at a time"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This is called a *discrete* representation.\n",
    "\n",
    "Discrete representations have a major drawbacks\n",
    "- they are long\n",
    "    -  $\\text{rep}(\\w)$ length is $||\\w|| * ||\\V||$\n",
    "- there is no meaningful metric of \"distance\" between the representation of words"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "To illustrate the \"lack of distance\" issue, let \n",
    "\n",
    "$$\n",
    "\\text{OHE}(w)\n",
    "$$\n",
    "\n",
    "denote the One Hot Encoding of word $w$.\n",
    "\n",
    "Using dot product (cosine similarity) as a measure of similarity\n",
    "\n",
    "| word   | OHE(word) | Similarity |\n",
    "| ---    | ---       | :---:        |\n",
    "| dog   | [1,0,0,0]   | OHE(word) $\\cdot$ OHE(dog)  = 1  |\n",
    "| dogs  | [0,1,0,0]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n",
    "| cat   | [0,0,1,0]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n",
    "| apple | [0,0,0,1]   | OHE(word) $\\cdot$ OHE(dog)  = 0  |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Each pair of distinct words has 0 similarity\n",
    "- no recognition of plural form\n",
    "- no recognition of commonality (pets)\n",
    "\n",
    "This is due to the fact that only a single \"feature\" of the OHE is active (non-zero)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "However, it's possible that, in reality, there are many \"dimensions\" to a word, for example\n",
    "- singular/plural\n",
    "- entity type, e.g., Person\n",
    "- positive/negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- \"Cats\", \"Dogs\", \"Apples\"\n",
    "    - related by being plural form\n",
    "- \"Cat\", \"Dog\"\n",
    "    - related by being animals\n",
    "- \"good\", \"bad\"\n",
    "    - related by being \"opposites\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus it is not unreasonable to represent a word as a short *dense vector* of features \n",
    "- each feature (vector element) captures a concept\n",
    "- numeric value of element encodes the strength of the word's relation to the concept\n",
    "\n",
    "Ideally the features would be indepenent\n",
    "\n",
    "This is called a *continuous* word representation.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Doing math with words\n",
    "\n",
    "Let's explore the implication and power of dense vector representation of words.\n",
    "\n",
    "Let $\\v_w$ be the dense vector/embedding for word $w$\n",
    "- captures multiple aspects of a word\n",
    "- where each element of the vector is a nearly-independent aspect\n",
    "- then we can perform interesting mathematical manipulations on word vectors"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "| $w$   | $\\v_w$ |\n",
    "| ---    | ---       | \n",
    "| cat   | [.7, .5, .01 ]   \n",
    "| cats   | [.7, .5, .95 ]  \n",
    "| dog   | [.7, .2, .01 ]   \n",
    "| dogs   | [.7, .2, .95 ]\n",
    "| apple   | [.1, .4, .01 ]   \n",
    "| apples   | [.1, .4, .95 ]\n",
    "\n",
    "Does the last dimension encode \"plural form\" ?\n",
    "$$\n",
    "\\v_\\text{cats} - \\v_\\text{cat} \\approx \\v_\\text{dogs} - \\v_\\text{dog} \\approx \\v_\\text{apples} - \\v_\\text{apple}\n",
    "$$\n",
    "\n",
    "If so:\n",
    "$$\n",
    "\\v_\\text{apples} \\approx \\v_\\text{apple} + (\\v_\\text{cats} - \\v_\\text{cat})\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Word analogies\n",
    "\n",
    "king:man :: ? : queen\n",
    "\n",
    "\n",
    "Let\n",
    "- $\\v_w$ be the dense vector for word $w$\n",
    "- $d(\\v_{w}, \\v_{w'})$ be some measure of the distance between the two vectors $\\v_{w}, \\v_{w'}$\n",
    "    - e.g., ( $1 - \\text{cosine similarity}$ )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using the distance metric,  define the set of words in vocabulary $\\V$ that are \"closest\" to a word $w$.\n",
    "\n",
    "Let\n",
    "- $\\text{wv}_{n',d}(\\v_w)$ be the dense vectors of the $n'$ words in $\\V$ closest to word $w$\n",
    "$$\n",
    "\\text{wv}_{n',d}(\\v_w) = \\{ \\v_{w'} | \\text{rank}_V( d(\\v_{w}, \\v_{w'}) ) \\le n' \\}\n",
    "$$\n",
    "- $N_{n',d}(w)$ be the set of $n'$ words associated with $\\text{wv}_{n',d}(\\v_w)$\n",
    "\n",
    "\n",
    "$$\n",
    "N_{n',d}(w) = \\{ w' | w' \\in \\text{wv}_{n',d}(\\v_w) \\}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can define approximate equality of two words $w, w'$ if they are among the closest words \n",
    "\n",
    "$$\n",
    "w \\approx_{n',d} w' \\; \\; \\text{if } \\w' \\in N_{n',d}(w) \n",
    "$$\n",
    "\n",
    "That is: \n",
    "- word $w$ is approximately equal to word $w'$\n",
    "- if $w'$ is among the $n'$ words closest to $w$ according to distance metric $d$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Finally, we can define word analogies:\n",
    "\n",
    "a:b :: c:d\n",
    "\n",
    "means\n",
    "\n",
    "$$\n",
    "\\v_a - \\v_b  \\approx_{n',d}  \\v_c - \\v_d \n",
    "$$\n",
    "\n",
    "So to solve the word analogy for $c$:\n",
    "$$\n",
    "\\v_c \\approx_{n',d}  \\v_a - \\v_b + \\v_d\n",
    "$$\n",
    "\n",
    "To be concrete:\n",
    "$$\n",
    "\\v_\\text{king} - \\v_\\text{man} + \\v_\\text{woman} \\approx_{n',d} \\v_\\text{queen}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## GloVe: Pre-trained embeddings\n",
    "\n",
    "Fortunately, you don't have to create your own word-embeddings from scratch.\n",
    "\n",
    "There are a number of pre-computed embeddings freely available."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "GloVe is a family of word embeddings that have been trained on large corpra\n",
    "- GloVe6b\n",
    "    - Trained on 6 Billion tokens\n",
    "    - 400K words\n",
    "    - Corpus:  Wikipedia (2014) + GigaWord5 (version 5, news wires 1994-2010)\n",
    "    - Many different dense vector lengths to choose from\n",
    "        - 50, 100, 200, 300"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We will illustrate the power of word embeddings using GloVe6b vectors of length $100$.\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{king- man + woman} &  \\approx_{n',d} & \\text{queen } \\\\\n",
    "\\text{man - boy + girl} &  \\approx_{n',d} & \\text{woman } \\\\\n",
    "\\text{Paris - France + Germany} &  \\approx_{n',d} & \\text{Berlin } \\\\\n",
    "\\text{Einstein - science + art} &  \\approx_{n',d} & \\text{Picasso} \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "You can see that the dense vectors seem to encode \"concepts\", that we can manipulate mathematically."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "You may discover some unintended bias\n",
    "\n",
    "$\n",
    "\\begin{array}[llllll]\\\\\n",
    "\\text{doctor - man + woman} &  \\approx_{n',d} & \\text{nurse } \\\\\n",
    "\\text{mechanic  - man + woman} &  \\approx_{n',d} & \\text{teacher } \\\\\n",
    "\\end{array}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Domain specific embeddings\n",
    "\n",
    "Do we speak Wikipedia English in this room ?\n",
    "\n",
    "Here are the neighborhoods of some financial terms, according to GloVe:\n",
    "\n",
    "$\n",
    "\\begin{array}[lll]\\\\\n",
    "N(\\text{bull}) & =  & [ \\text{cow, elephant, dog, wolf, pit, bear, rider, lion, horse}] \\\\\n",
    "N(\\text{short}) & =  & [ \\text{rather, instead, making, time, though, well, longer, shorter, long}] \\\\\n",
    "N(\\text{strike}) & =  & [ \\text{workers, struck, action, blow, striking, protest, stoppage, walkout, strikes}] \\\\\n",
    "N(\\text{FX}) & =  & [ \\text{showtime, cnbc, ff, nickelodeon, hbo, wb, cw, vh1}] \\\\\n",
    "\\end{array}\n",
    "$\n",
    "\n",
    "It may be desirable to create word embeddings on a narrow (domain specific) corpus.\n",
    "\n",
    "This is not difficult provided you have enough data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "# Obtaining Dense Vectors: Transfer Learning\n",
    "\n",
    "How do we obtain Dense Vector representation of words ?\n",
    "\n",
    "We learn them !"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Suppose we had a task T \n",
    "that involves mapping a sequence of words to an outcome.\n",
    "\n",
    "To be concrete: mapping a movie review to an indicator of Positive/Negative sentiment.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Ignoring for the moment the issue of converting variable length sequences to a fixed length\n",
    "- inputs are OHE of words\n",
    "- target is Positive/Negative label\n",
    "\n",
    "- Logistic Regression from  sentence representation to binary target Positive/Negative"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "One could also ask\n",
    "- can we map the OHE of a word $\\w_\\tp$ (length $|\\V|$)\n",
    "- to a shorter, dense vector $\\mathbf{e}_\\tp$ of length $n_e$\n",
    "- and use the dense vector in the Logistic Regerssion\n",
    " \n",
    "This mapping can be represented by an an $(|\\V| \\times n_e)$ matrix\n",
    "$\\E$ \n",
    "\n",
    "$$\n",
    "\\mathbf{e}_\\tp = \\text{OHE}(\\w_\\tp)^T \\E \n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Using Machine Learning, \n",
    "- we solve for both the Logistic Regression parameters $\\W$ *and* $\\mathbf{E}$\n",
    "- when solving the Classification Task via Logistic Regression.\n",
    "\n",
    "The matrix $\\mathbf{E}$ is called \n",
    "- an *embedding matrix* for words \n",
    "- and\n",
    "$\\e_\\tp$ is called an *embedding*  or *word vector* for word $\\w_\\tp$.\n",
    "\n",
    "*Word embeddings* have become an important component of Deep Learning for NLP."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>Word prediction: Neural Net</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=\"images/w2v_word_prediction_layers.jpg\" width=800></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In other words\n",
    "- we have learned a dense vector representation of words $\\mathbf{E}$\n",
    "- that is useful for a particular classification task\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Might it be possible that the dense vector representation of words for this task\n",
    "- is useful for other tasks involving words ?\n",
    "- this is Transfer Learning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.594px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
