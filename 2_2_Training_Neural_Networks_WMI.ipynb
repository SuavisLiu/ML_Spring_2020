{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [
    {
     "data": {
      "text/latex": [
       "$$\n",
       "\\newcommand{\\x}{\\mathbf{x}}\n",
       "\\newcommand{\\tx}{\\tilde{\\x}}\n",
       "\\newcommand{\\y}{\\mathbf{y}}\n",
       "\\newcommand{\\b}{\\mathbf{b}}\n",
       "\\newcommand{\\c}{\\mathbf{c}}\n",
       "\\newcommand{\\e}{\\mathbf{e}}\n",
       "\\newcommand{\\z}{\\mathbf{z}}\n",
       "\\newcommand{\\h}{\\mathbf{h}}\n",
       "\\newcommand{\\u}{\\mathbf{u}}\n",
       "\\newcommand{\\v}{\\mathbf{v}}\n",
       "\\newcommand{\\w}{\\mathbf{w}}\n",
       "\\newcommand{\\W}{\\mathbf{W}}\n",
       "\\newcommand{\\X}{\\mathbf{X}}\n",
       "\\newcommand{\\KL}{\\mathbf{KL}}\n",
       "\\newcommand{\\E}{{\\mathbb{E}}}\n",
       "\\newcommand{\\ip}{\\mathbf{{(i)}}}\n",
       "%\n",
       "% Test set\n",
       "\\newcommand{\\xt}{\\underline{\\x}}\n",
       "\\newcommand{\\yt}{\\underline{\\y}}\n",
       "\\newcommand{\\Xt}{\\underline{\\X}}\n",
       "\\newcommand{\\perfm}{\\mathcal{P}}\n",
       "%\n",
       "% \\ll indexes a layer; we can change the actual letter\n",
       "\\newcommand{\\ll}{l}\n",
       "\\newcommand{\\llp}{{(\\ll)}}\n",
       "%\n",
       "\\newcommand{Thetam}{\\Theta_{-0}}\n",
       "\n",
       "%\n",
       "% \\tt indexes a time step\n",
       "\\newcommand{\\tt}{t}\n",
       "\\newcommand{\\tp}{{(\\tt)}}\n",
       "%\n",
       "\\newcommand{\\loss}{\\mathcal{L}}\n",
       "\\newcommand{\\cost}{\\mathcal{L}}\n",
       "%\n",
       "%                     \n",
       "\\newcommand{\\pdata}{p_\\text{data}}\n",
       "\\newcommand{\\pmodel}{p_\\text{model}}\n",
       "%\n",
       "% SVM\n",
       "\\newcommand{\\margin}{{\\mathbb{m}}}\n",
       "\\newcommand{\\lmk}{\\boldsymbol{\\ell}}\n",
       "%\n",
       "% Functions with arguments\n",
       "\\def\\xsy#1#2{#1^#2}\n",
       "\\def\\rand#1{\\tilde{#1}}\n",
       "\\def\\randx{\\rand{\\x}}\n",
       "\\def\\randy{\\rand{\\y}}\n",
       "\\def\\trans#1{\\dot{#1}}\n",
       "\\def\\transx{\\trans{\\x}}\n",
       "\\def\\transy{\\trans{\\y}}\n",
       "%\n",
       "\\def\\argmax#1{\\underset{#1} {\\operatorname{argmax}} }\n",
       "\\def\\argmin#1{\\underset{#1} {\\operatorname{argmin}} }\n",
       "\\def\\max#1{\\underset{#1} {\\operatorname{max}} }\n",
       "\\def\\min#1{\\underset{#1} {\\operatorname{min}} }\n",
       "%\n",
       "\\def\\pr#1{\\mathcal{p}(#1)}\n",
       "\\def\\prc#1#2{\\mathcal{p}(#1 \\; | \\; #2)}\n",
       "\\def\\cnt#1{\\mathcal{count}_{#1}}\n",
       "\\def\\node#1{\\mathbb{#1}}\n",
       "%\n",
       "\\newcommand{\\floor}[1]{\\left\\lfloor #1 \\right\\rfloor}\n",
       "\\newcommand{\\ceil}[1]{\\left\\lceil #1 \\right\\rceil}\n",
       "%\n",
       "\\def\\loc#1{{\\text{##} {#1}}}\n",
       "%\n",
       "$$\n"
      ],
      "text/plain": [
       "<IPython.core.display.Latex object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "%run Latex_macros.ipynb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "# My standard magic !  You will see this in almost all my notebooks.\n",
    "\n",
    "from IPython.core.interactiveshell import InteractiveShell\n",
    "InteractiveShell.ast_node_interactivity = \"all\"\n",
    "\n",
    "# Reload all modules imported with %aimport\n",
    "%load_ext autoreload\n",
    "%autoreload 1\n",
    "\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "slideshow": {
     "slide_type": "notes"
    }
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import neural_net_helper\n",
    "%aimport neural_net_helper\n",
    "\n",
    "nnh = neural_net_helper.NN_Helper()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Back propagation\n",
    "\n",
    "Gradient Decent works by updating weights $\\W_\\llp$ by the derivative of the loss $\\loss$ with respect to $\\W_\\llp$.\n",
    "\n",
    "We will now show how the derivatives \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} \\,\\text{for} \\; \\ll=1, \\ldots, L\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "are computed, first mathematically and then by code."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can do this via a procedure know as back propagation\n",
    "\n",
    "It is really nothing more than an *iterated* application of the Chain Rule of Calculus.\n",
    "\n",
    "Let \n",
    "- $\\loss$ denote loss (computed after final layer $L$)\n",
    "- $\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial y_\\llp}$ denote the derivative of $\\loss$ with respect to the output of layer $\\ll$, i.e., $y_\\llp$,\n",
    "    - refer to as **loss gradient** (at output of layer $\\ll$)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We will show how to compute \n",
    "- $\\frac{\\partial \\loss}{\\partial W_\\llp}$, from $\\loss'_\\llp$  for $\\ll \\in [1,L]$\n",
    "\n",
    "We will also show how to compute\n",
    "- $\\loss'_{(\\ll-1)}$ from $\\loss'_\\ll$ \n",
    "    - so that we can continue this process as the previous layer (i.e, *propogate loss gradient backwards*)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Note that $\\y_\\llp$ is a function of $\\y_{(\\ll-1)}$ (the output of the previous layer) and $\\W_\\llp$, the parameters of layer $\\ll$.\n",
    "\n",
    "We can compute derivatives of $\\y_\\llp$ with respect to each of its inputs\n",
    "- $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "- $\\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}$\n",
    "\n",
    "Refer to these as **local gradients**\n",
    "\n",
    "Note that we can compute the local gradients during the **forward pass** as the derivatives only depend on inputs and not on any value subsequent to layer $\\ll$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Recall we defined the loss as being pseudo layer $L+1$\n",
    "$$\\y_{(L+1)} = \\loss\n",
    "$$\n",
    "\n",
    "Then $\\loss'_{(L+1)} = 1$ and this is the base for our backwards computation of $\\loss'_{\\llp}, \\ll < L+1$.\n",
    "\n",
    "Similarly we consider the input $\\x$ to be the \"output\" of layer $0$: $\\y_{(0)} = \\x$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "We can use the chain rule to obtain the \n",
    "- gradient with respect to weights $\\W_\\llp$, given the loss gradient $\\loss'_\\llp$ \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial \\W_\\llp} & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp} & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\W_\\llp}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is: \n",
    "- gradient of $\\loss$ with respect to weight $\\W_\\llp$ \n",
    "- is the loss gradient (at current step), multiplied by\n",
    "- a local gradient (with respect to input  $W_\\llp$ )\n",
    "\n",
    "So we have the information required to update $\\W_\\llp$ by Gradient Descent.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Now that we have gradients for layer $\\ll$, we need to work backward to layer $\\ll -1$.\n",
    "\n",
    "For the loss gradient:\n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\loss'_{(\\ll-1)} & = & \\frac{\\partial \\loss}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\frac{\\partial \\loss}{\\partial \\y_\\llp} \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = & \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "That is, the loss gradient of the preceding layer is the product of\n",
    "- the loss gradient of the current layer\n",
    "- the local gradient with respect to the layer's inputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This \"backwards\" computation may seem odd.\n",
    "\n",
    "After all, when we encounter layer $\\ll$ on the forward pass we have **no idea**\n",
    "- what the subsequent layers look like\n",
    "- what the loss (which depends on $\\y_{(L)}$) will be\n",
    "\n",
    "\n",
    "None the less:\n",
    "- all you need to know is we compute $\\loss'_\\llp$ from deep $\\ll$ to shallow\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Vanishing/exploding gradients\n",
    "\n",
    "Now that we have a better view of how backward propagation of gradients work, we are equipped\n",
    "to understand the difficulties of training the weights.\n",
    "\n",
    "Until the problems were understood, and solutions found, the evolution of Deep Learning\n",
    "was extremely slow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's summarize back propagation up until this point\n",
    "- We compute the loss gradient $\\loss'_\\llp = \\frac{\\partial \\loss}{\\partial \\y_\\llp}$ of each layer $\\ll$ in descending order\n",
    "\n",
    "- The backward step  to compute the loss gradient of the preceding layer is:  \n",
    "    - $\\loss'_{(\\ll-1)} =  \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "\n",
    "When we derived back propagation, we didn't look \"inside\" of the \"local gradient \" $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}}$\n",
    "\n",
    "We will do so now."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's look more deeply into the  term $\\frac{\\partial \\y_\\llp}{\\partial \\y_{(i-1)}}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "$$\n",
    "\\begin{array}[lllll] \\\\\n",
    "\\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp))}{\\partial \\y_{(\\ll-1)}} & (\\text{def. of } \\y_\\llp) \\\\\n",
    "                                      & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, W_\\llp) )}{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)} \\frac{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)}{\\partial \\y_{(\\ll-1)}} &  (\\text{chain rule}) \\\\\n",
    "                                      & = a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "where we define\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "a'_\\llp & = & \\frac{\\partial a_\\llp ( f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp) )}{\\partial f_\\llp(\\y_{(\\ll-1)}, \\W_\\llp)} \\\\\n",
    "f'_\\llp & = & \\frac{\\partial f_\\llp(\\y_{(\\ll-1)}, W_\\llp)}{\\partial \\y_{(\\ll-1)}} \n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "$a'_\\llp$ is the derivative of activation function $a_\\llp$.\n",
    "\n",
    "We won't expicitly write it out other than to observe $a'_\\llp \\in [0,1]$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "\n",
    "Substituting the value of the loss gradient into the backward update rule:\n",
    "\n",
    "$$\n",
    "\\begin{array}[llll]\\\\\n",
    "\\loss'_{(\\ll-1)} & = &  \\loss'_\\llp \\frac{\\partial \\y_\\llp}{\\partial \\y_{(\\ll-1)}} \\\\\n",
    "         & = &  \\loss'_\\llp a'_\\llp f'_\\llp\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "Hopefuly, you can see that if iterate through single backward steps, we can derive\n",
    "an expression for the loss gradient at layer $\\ll$ in terms of the loss gradient\n",
    "of the final layer $K$:\n",
    "\n",
    "Since\n",
    "$$\\loss'_\\llp  =   \\loss'_{(\\ll+1)} \\frac{\\partial \\y_{(\\ll+1)}}{\\partial \\y_\\llp}$$\n",
    "\n",
    "we get\n",
    "$$\\loss'_\\llp  =   \\loss'_{(L+1)} \\prod_{l'=i+1}^L  a'_{(l')} f'_{(l')}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The issue is that, since \n",
    "$$\n",
    "0 \\le a'_\\llp \\le \\max{z} a'_\\llp(z)\n",
    "$$\n",
    "\n",
    "the product \n",
    "$$\\prod_{l'=i+1}^K {a'_{(l')}}\n",
    "$$\n",
    "can be increasingly small as the number of layers $K$ grows, if $\\max{z} a'_\\llp(z) < 1$.\n",
    "\n",
    "Note, for $a_\\llp = \\sigma$ (the sigmoid function), $\\max{z} a'_\\llp(z) = 0.25$  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus, unless offset by the $f'_\\llp$ terms, $\\loss'_\\llp$ will quickly diminish to $0$ as $K$ decreases,\n",
    "i.e., as we seek to compute $\\loss'_\\llp$ for layers $\\ll$ closest to the input.\n",
    "This means \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss}{\\partial W_\\llp} & = & \\frac{\\partial \\loss}{\\partial y_\\llp} \\frac{\\partial y_\\llp}{\\partial W_\\llp} & = & \\loss'_\\llp \\frac{\\partial y_\\llp}{\\partial W_\\ip}\n",
    "\\end{array}\n",
    "$$\n",
    "will approach $0$.\n",
    "Since this term is used in the update to $W_\\ip$, we won't learn weights for the earliest layers."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "We can now diagnose one reason that training of early Deep Learning networks was difficult\n",
    "- use of sigmoid activations were common (inspired by biology)\n",
    "- if activations were very large/small, we are in a region where the sigmoid's derivatives are $0$\n",
    "- even when non-zero,the maximum of the derivative of the sigmoid is much smaller than $1$\n",
    "- the end result was that deep networks suffered from Vanishing Gradients\n",
    "\n",
    "The ReLU function's derivative does not suffer from this problem and ReLU's now tend to be\n",
    "the standard activation (barring other considerations, such as the range of outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# What took so long ?\n",
    "\n",
    "Why did it take so long for \n",
    "Deep Learning to achieve success.\n",
    "\n",
    "An historical perspective:\n",
    "\n",
    "- Perceptron invented 1957\n",
    "- mid-1970's: First \"AI Winter\"\n",
    "- Late 1980's: secibd \"AI Winter\"\n",
    "- 2010: Re-emergence of AI"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "The promise of AI led to great expectations, that were ultimately unfulfilled.\n",
    "The difficulty was the inability to train networks.\n",
    "\n",
    "We now spend some time investigating the causes, and solutions, to the difficulty of training networks.\n",
    "\n",
    "Broadly speaking the issues are\n",
    "- proper scaling of the inputs\n",
    "- initialization of learnable weights\n",
    "- making sure that the proper scaling of inputs continues to each layer, not just the input\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Proper scaling of inputs\n",
    "\n",
    "## Importance of zero centered inputs (for each layer)\n",
    "[Efficient Backprop paper, LeCunn98](http://yann.lecun.com/exdb/publis/pdf/lecun-98b.pdf)\n",
    "\n",
    "**Zero centered** means average (over the training set) value of each feature of examples is mean $0$.\n",
    "\n",
    "Gradient descent updates each element of a layer $\\ll$'s weights $\\W_\\llp$ by\n",
    "the per-example losses \n",
    "\n",
    "$$\n",
    "\\begin{array}[lll] \\\\\n",
    "\\frac{\\partial \\loss^\\ip }{\\partial W_\\llp} & = & \\frac{\\partial \\loss^\\ip}{\\partial \\y_\\llp^\\ip} \\frac{\\partial \\y_\\llp^\\ip}{\\partial \\W_\\llp} \n",
    "\\end{array}\n",
    "$$\n",
    "summed over examples $i$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Over-simplifying:\n",
    "- the local derivative is proportional to the input:\n",
    "$$\n",
    "\\frac{ \\partial{\\y_\\llp^\\ip} } { \\partial \\W_\\llp } = a'_\\llp \\y_{(\\ll-1)}^\\ip\n",
    "$$\n",
    "for FC $y_\\llp = a_\\llp ( \\y_{(\\ll-1)} \\W_\\llp )$.\n",
    "- thus the updates of $\\W_{\\llp,j}$ will be biased by $\\bar{\\y}_{(\\ll-1),j}$ = the average (over examples $i$) of $\\y_{(\\ll-1),j}^\\ip$\n",
    "- for $\\ll = 1$, this is the average of the input feature $\\x_j$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "In the particular case that each feature $j$'s average $\\bar{\\x}_j$ has the same sign:\n",
    "- updates in all dimensions will have the same sign\n",
    "- this can result in an indirect \"zig-zag\" toward the optimum\n",
    "    - Exampe: two dimensions: \n",
    "        - We can navigate the loss surface north-east or south-west only ! \n",
    "        - To get to a point north-west from the current, we have to zig-zag.\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "- Note that this is an issue for *all* layers, not just layer $\\ll =1$.\n",
    "\n",
    "- Also note: the problem is compounded by activations whose outputs are not zero-centered (e.g., ReLU, sigmoid)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Importance of unit variance inputs (weight initialization)\n",
    "\n",
    "The same argument we made for zero-centering a feature can be extended to it's variance:\n",
    "- the variance of feature $j$ over all trainng examples $i$ is the varaince of $\\y_{(\\ll-1),j}$\n",
    "\n",
    "If the variance of features $j$ and $j'$ are different, their updates will happen at different rates.\n",
    "\n",
    "We will examine this in greater depth during our discussion of weight initialization.\n",
    "\n",
    "For now: it is desirable that the input to *each* layer have it's features somewhat normalized.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# Initialization\n",
    "\n",
    "Training is all about discovering good weights.\n",
    "\n",
    "As prosaic as it sounds: how do we *initialize* the weights before training ?\n",
    "Does it matter ?\n",
    "\n",
    "It turns out that the choice of initial weights does matter.\n",
    "\n",
    "Let's start with some *bad* choices"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Bad choices\n",
    "\n",
    "### Too big/small\n",
    "\n",
    "Layers usually consist of linear operations (e.g., matrix multiplication and addition of bias)\n",
    "followed by a non-linear activation.\n",
    "The range of many activation functions includes large regions where the derivatives are near zero,\n",
    "usually corresponding to very large/small activations.\n",
    "\n",
    "The SGD update rule uses the magnitude of the gradient to update weights.\n",
    "Obviously, if the gradients are all near-0, learning cannot occur.\n",
    "\n",
    "So one bad choice is any set of weights that tends to push activations to regions of the non-linear\n",
    "activation with zero gradient."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### Identical weights\n",
    "\n",
    "Consider layer $\\ll$ with $n_\\ll$ units (neurons) implementing identical operations (e.g. FC + ReLu).\n",
    "\n",
    "Let  $\\W_{\\llp, k}$ denote the weights of unit $k$.\n",
    "\n",
    "Suppose we initialized the weights (and biases) of all units to the *same* vector.\n",
    "$$\n",
    "\\W_{\\llp, k} = \\w_\\llp, \\; 1 \\le k \\le n_\\ll\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Consider two neuron $j, j'$ in the same layer $\\ll$\n",
    "$$\n",
    "\\begin{array}[lll]\\\\\n",
    "\\y_{\\llp, j}  & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\y_{\\llp, j'} & = & a_\\llp ( \\w_\\llp \\y_{(\\ll-1)} + \\b_\\llp ) \\\\\n",
    "\\end{array}\n",
    "$$\n",
    "\n",
    "- Both neuron will compute the same activation\n",
    "- Both neurons will have the same gradient\n",
    "- Both neurons will have the same weight update\n",
    " \n",
    "Thus, the weights in layer $i$ will start off identical and will remain identical due to identical updates.\n",
    "\n",
    "So identical initialization will lead to a failure for individual neurons to learn different features.\n",
    "\n",
    "Many approaches use some for of random initialization to break the symmetry we just described."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Glorot initialization\n",
    "\n",
    "We have previousy shown that each element $j$ of the first input layer ($\\x_{(0),j}$) should\n",
    "have unit variance across the training set.  \n",
    "\n",
    "This was meant to ensure that the first layer's weights\n",
    "updated at the same rate and that the activations of the first layer fell into regions of the activation\n",
    "function that had non-zero gradients.\n",
    "\n",
    "But this is not enough."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Let's assume for the moment that each element $j$ of the input vector $\\y_{(\\ll-1)}$ is mean $0$, unit variance\n",
    "and mutually independent.  \n",
    "\n",
    "So view each $\\y_{(\\ll-1),j}$ as an independent random variable with mean $0$\n",
    "and unit variance.  Furthermore, let's assome each element $\\W_{\\llp,j}$ is similarly distributed.\n",
    "\n",
    "Consider the matrix multiplication in layer $\\ll$ involving the $n_{\\ll-1}$ output of layer $\\ll$.\n",
    "$$f_\\llp(\\y_{(\\ll-1)}, W_\\llp) = \\y_{(\\ll-1)} \\cdot W_\\llp$$\n",
    "\n",
    "This expression is the weighted sum over $j$ of the product of\n",
    "- a mean 0, unit variance random variable $\\y_{(\\ll-1),j}$\n",
    "- a mean 0, unit variancerandom variable  $\\W_{\\llp,j}$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "For two random variables $X, Y$, the variance of the product \n",
    "[is](https://en.wikipedia.org/wiki/Variance#Product_of_independent_variables)\n",
    "\n",
    "$$\n",
    "\\text{Var}(XY) = \\mathbb{E}(X)^2 \\text{Var}(Y) + \\mathbb{E}(Y)^2 \\text{Var}(X) + \\text{Var}(X)\\text{Var}(Y)\n",
    "$$\n",
    "\n",
    "So \n",
    "$$\\text{Var}(\\y_{(\\ll-1),j} \\W_{\\llp,j}) = 0^2 * 1 + 0^2 * 1 + 1 * 1 = 1\n",
    "$$\n",
    "\n",
    "Thus the variance of the dot product of $n_{\\ll-1}$ products is $n_{\\ll-1}$, not $1$ as desired.\n",
    "\n",
    "However, by scaling each $\\W_{\\llp,j}$ by \n",
    "$$\n",
    "\\frac{1}{\\sqrt{n_{\\ll-1}}}\n",
    "$$\n",
    "the variance becomes $1$."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "*Glorot* (also called *Xavier*) initialization sets the initial weights to a number drawn from \n",
    "mean $0$, unit variance distribution (either normal or uniform)\n",
    "$\\frac{1}{\\sqrt{n_{\\ll-1}}}\n",
    "$.\n",
    "\n",
    "Note that we don't strictly need the requirement of unit variance -- it works equally well as long\n",
    "as the input and output variances are the same, which is the goal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "This only partially solves the problem as it only ensures unit variance of the input to the activation function.\n",
    "\n",
    "The [original Glorot paper](http://proceedings.mlr.press/v9/glorot10a/glorot10a.pdf) justifies this\n",
    "by assuming either a $\\tanh$ or sigmoid activation functions and these functions can be approximated\n",
    "as linear in the active region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "Thus far, we have achieved unit variance during the forward pass.\n",
    "During back propagation, it was shown that the scaling factor depends on the number of outputs\n",
    "of layer $\\ll$, rather than number of inputs, so the scaling factor needs to be\n",
    "$\\frac{1}{\\sqrt{n_\\ll}}\n",
    "$\n",
    "\n",
    "Taking the average of the two scaling factors gives a final factor of\n",
    "$\\frac{1}{\\sqrt{ \\frac{ n_{\\ll-1} + n_\\ll}{2} } } = \\sqrt{\\frac{2}{n_{\\ll-1} + n_\\ll}}\n",
    "$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## Kaiming/He initialization\n",
    "\n",
    "[Kaiming et al](https://arxiv.org/pdf/1502.01852.pdf) extended the results of Glorot et. al\n",
    "to the ReLU activation."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The ReLU activation has two distinct regions: one linear (for inputs greater than 0) and one all zero.\n",
    "\n",
    "The linear region of the activation corresponds to the assumption of the Glorot method.\n",
    "\n",
    "So if inputs to the ReLU are equally distributed around 0, this is approximately the same\n",
    "as the Glorot method with half the number of inputs.\n",
    "- that is: half of the ReLU's will be in the active region and half will be in the inactive region."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "The Kaiming scaling factor is thus:\n",
    "$$\n",
    "\\sqrt{\\frac{2}{n_{(\\ll-1)}} }\n",
    "$$\n",
    "in order to preserve unit variance."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "# How big should my NN be ?\n",
    "\n",
    "There is a paradox in building Neural Networks:\n",
    "- Start off training an overly large NN (many units)\n",
    "- Many units turn out to be \"dead\": near zero weights\n",
    "- Reduce the number of units\n",
    "- Can't train !\n",
    "\n",
    "Given a fixed number of layers: it is easier to train a big NN than a small one.\n",
    "\n",
    "\"Somewhere in this big mess must be something valuable\"\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>\"Big\" NN with dead nodes</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=images/Dropout_NN_wo_dropout.jpg width=400></td>\n",
    "    </tr>\n",
    "</table>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "<table>\n",
    "    <tr>\n",
    "        <th><center>\"Big\" NN after dead nodes have been pruned</center></th>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td><img src=images/Dropout_NN_w_dropout.jpg width=400></td>\n",
    "    </tr>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "[The Lottery Ticket Hypothesis](https://arxiv.org/abs/1803.03635)\n",
    "is an interesting paper that addresses this issue.\n",
    "\n",
    "For now:\n",
    "- Use bigger than necessary NN's\n",
    "- With regularization to \"prune\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Done\n"
     ]
    }
   ],
   "source": [
    "print(\"Done\")"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "370.547px"
   },
   "toc_section_display": true,
   "toc_window_display": true
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
